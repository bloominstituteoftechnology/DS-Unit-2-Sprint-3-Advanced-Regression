{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS1_234_Ridge_Regression_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyasJothish/DS-Unit-2-Sprint-3-Advanced-Regression/blob/master/module4-ridge-regression/LS_DS1_234_Ridge_Regression_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-eFju4_DDKeX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lambda School Data Science - Ridge Regression\n",
        "\n",
        "Regularize your way to a better tomorrow."
      ]
    },
    {
      "metadata": {
        "id": "5v5cBm19JxOj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lecture\n",
        "\n",
        "Data science depends on math, and math is generally focused on situations where:\n",
        "\n",
        "1. a solution exists,\n",
        "2. the solution is unique,\n",
        "3. the solution's behavior changes continuously with the initial conditions.\n",
        "\n",
        "These are known as [well-posed problems](https://en.wikipedia.org/wiki/Well-posed_problem), and are the sorts of assumptions so core in traditional techniques that it is easy to forget about them. But they do matter, as there can be exceptions:\n",
        "\n",
        "1. no solution - e.g. no $x$ such that $Ax = b$\n",
        "2. multiple solutions - e.g. several $x_1, x_2, ...$ such that $Ax = b$\n",
        "3. \"chaotic\" systems - situations where small changes in initial conditions interact and reverberate in essentially unpredictable ways - for instance, the difficulty in longterm predictions of weather (N.B. not the same thing as longterm predictions of *climate*) - you can think of this as models that fail to generalize well, because they overfit on the training data (the initial conditions)\n",
        "\n",
        "Problems suffering from the above are called ill-posed problems. Relating to linear algebra and systems of equations, the only truly well-posed problems are those with a single unique solution.\n",
        "\n",
        "![Intersecting lines](https://upload.wikimedia.org/wikipedia/commons/c/c0/Intersecting_Lines.svg)\n",
        "\n",
        "Think for a moment - what would the above plot look like if there was no solution? If there were multiple solutions? And how would that generalize to higher dimensions?\n",
        "\n",
        "A lot of what you covered with linear regression was about getting matrices into the right shape for them to be solvable in this sense. But some matrices just won't submit to this, and other problems may technically \"fit\" linear regression but still be violating the above assumptions in subtle ways.\n",
        "\n",
        "[Overfitting](https://en.wikipedia.org/wiki/Overfitting) is in some ways a special case of this - an overfit model uses more features/parameters than is \"justified\" by the data (essentially by the *dimensionality* of the data, as measured by $n$ the number of observations). As the number of features approaches the number of observations, linear regression still \"works\", but it starts giving fairly perverse results. In particular, it results in a model that fails to *generalize* - and so the core goal of prediction and explanatory power is undermined.\n",
        "\n",
        "How is this related to well and ill-posed problems? It's not clearly a no solution or multiple solution case, but it does fall in the third category - overfitting results in fitting to the \"noise\" in the data, which means the particulars of one random sample or another (different initial conditions )will result in dramatically different models.\n",
        "\n",
        "## Stop and think - what are ways to address these issues?\n",
        "\n",
        "Let's examine in the context of housing data."
      ]
    },
    {
      "metadata": {
        "id": "TDh_Oz9HDHeR",
        "colab_type": "code",
        "outputId": "68e7c087-67ad-4293-cb36-d0a11931a773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "boston = load_boston()\n",
        "boston.data = scale(boston.data)  # Very helpful for regularization!\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "df['Price'] = boston.target\n",
        "df.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.419782</td>\n",
              "      <td>0.284830</td>\n",
              "      <td>-1.287909</td>\n",
              "      <td>-0.272599</td>\n",
              "      <td>-0.144217</td>\n",
              "      <td>0.413672</td>\n",
              "      <td>-0.120013</td>\n",
              "      <td>0.140214</td>\n",
              "      <td>-0.982843</td>\n",
              "      <td>-0.666608</td>\n",
              "      <td>-1.459000</td>\n",
              "      <td>0.441052</td>\n",
              "      <td>-1.075562</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.417339</td>\n",
              "      <td>-0.487722</td>\n",
              "      <td>-0.593381</td>\n",
              "      <td>-0.272599</td>\n",
              "      <td>-0.740262</td>\n",
              "      <td>0.194274</td>\n",
              "      <td>0.367166</td>\n",
              "      <td>0.557160</td>\n",
              "      <td>-0.867883</td>\n",
              "      <td>-0.987329</td>\n",
              "      <td>-0.303094</td>\n",
              "      <td>0.441052</td>\n",
              "      <td>-0.492439</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.417342</td>\n",
              "      <td>-0.487722</td>\n",
              "      <td>-0.593381</td>\n",
              "      <td>-0.272599</td>\n",
              "      <td>-0.740262</td>\n",
              "      <td>1.282714</td>\n",
              "      <td>-0.265812</td>\n",
              "      <td>0.557160</td>\n",
              "      <td>-0.867883</td>\n",
              "      <td>-0.987329</td>\n",
              "      <td>-0.303094</td>\n",
              "      <td>0.396427</td>\n",
              "      <td>-1.208727</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.416750</td>\n",
              "      <td>-0.487722</td>\n",
              "      <td>-1.306878</td>\n",
              "      <td>-0.272599</td>\n",
              "      <td>-0.835284</td>\n",
              "      <td>1.016303</td>\n",
              "      <td>-0.809889</td>\n",
              "      <td>1.077737</td>\n",
              "      <td>-0.752922</td>\n",
              "      <td>-1.106115</td>\n",
              "      <td>0.113032</td>\n",
              "      <td>0.416163</td>\n",
              "      <td>-1.361517</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.412482</td>\n",
              "      <td>-0.487722</td>\n",
              "      <td>-1.306878</td>\n",
              "      <td>-0.272599</td>\n",
              "      <td>-0.835284</td>\n",
              "      <td>1.228577</td>\n",
              "      <td>-0.511180</td>\n",
              "      <td>1.077737</td>\n",
              "      <td>-0.752922</td>\n",
              "      <td>-1.106115</td>\n",
              "      <td>0.113032</td>\n",
              "      <td>0.441052</td>\n",
              "      <td>-1.026501</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
              "0 -0.419782  0.284830 -1.287909 -0.272599 -0.144217  0.413672 -0.120013   \n",
              "1 -0.417339 -0.487722 -0.593381 -0.272599 -0.740262  0.194274  0.367166   \n",
              "2 -0.417342 -0.487722 -0.593381 -0.272599 -0.740262  1.282714 -0.265812   \n",
              "3 -0.416750 -0.487722 -1.306878 -0.272599 -0.835284  1.016303 -0.809889   \n",
              "4 -0.412482 -0.487722 -1.306878 -0.272599 -0.835284  1.228577 -0.511180   \n",
              "\n",
              "        DIS       RAD       TAX   PTRATIO         B     LSTAT  Price  \n",
              "0  0.140214 -0.982843 -0.666608 -1.459000  0.441052 -1.075562   24.0  \n",
              "1  0.557160 -0.867883 -0.987329 -0.303094  0.441052 -0.492439   21.6  \n",
              "2  0.557160 -0.867883 -0.987329 -0.303094  0.396427 -1.208727   34.7  \n",
              "3  1.077737 -0.752922 -1.106115  0.113032  0.416163 -1.361517   33.4  \n",
              "4  1.077737 -0.752922 -1.106115  0.113032  0.441052 -1.026501   36.2  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "bx1Xtp_x3Etx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "?scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3u24Yr-SkIhb",
        "colab_type": "code",
        "outputId": "732c6ca3-0db1-4576-9cb8-9a547b35ab3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "0vlZShpFkll2",
        "colab_type": "code",
        "outputId": "0403665f-67f8-46ba-9538-f18caeed269e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Let's try good old least squares!\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X = df.drop('Price', axis='columns')\n",
        "y = df.Price\n",
        "\n",
        "lin_reg = LinearRegression().fit(X, y)\n",
        "mean_squared_error(y, lin_reg.predict(X))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.894831181729206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "erOFuJKWlTad",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That seems like a pretty good score, but...\n",
        "\n",
        "![Kitchen Sink](https://i.imgur.com/ZZxqhT1.jpg)\n",
        "\n",
        "Chances are this doesn't generalize very well. You can verify this by splitting the data to properly test model validity."
      ]
    },
    {
      "metadata": {
        "id": "CG6DZ1UcqbEx",
        "colab_type": "code",
        "outputId": "fdce97c3-4f8e-4278-8e84-5d3a7da7af9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)\n",
        "lin_reg_split = LinearRegression().fit(X_train, y_train)\n",
        "print(mean_squared_error(y, lin_reg_split.predict(X)))\n",
        "print(mean_squared_error(y_test, lin_reg_split.predict(X_test)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22.347018673376052\n",
            "26.273991426429014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ILHGe53Iqehg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Oops! ðŸ’¥\n",
        "\n",
        "### What can we do?\n",
        "\n",
        "- Use fewer features - sure, but it can be a lot of work to figure out *which* features, and (in cases like this) there may not be any good reason to really favor some features over another.\n",
        "- Get more data! This is actually a pretty good approach in tech, since apps generate lots of data all the time (and we made this situation by artificially constraining our data). But for case studies, existing data, etc. it won't work.\n",
        "- **Regularize!**\n",
        "\n",
        "## Regularization just means \"add bias\"\n",
        "\n",
        "OK, there's a bit more to it than that. But that's the core intuition - the problem is the model working \"too well\", so fix it by making it harder for the model!\n",
        "\n",
        "It may sound strange - a technique that is purposefully \"worse\" - but in certain situations, it can really get results.\n",
        "\n",
        "What's bias? In the context of statistics and machine learning, bias is when a predictive model fails to identify relationships between features and the output. In a word, bias is *underfitting*.\n",
        "\n",
        "We want to add bias to the model because of the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) - variance is the sensitivity of a model to the random noise in its training data (i.e. *overfitting*), and bias and variance are naturally (inversely) related. Increasing one will always decrease the other, with regards to the overall generalization error (predictive accuracy on unseen data).\n",
        "\n",
        "Visually, the result looks like this:\n",
        "\n",
        "![Regularization example plot](https://upload.wikimedia.org/wikipedia/commons/0/02/Regularization.svg)\n",
        "\n",
        "The blue line is overfit, using more dimensions than are needed to explain the data and so much of the movement is based on noise and won't generalize well. The green line still fits the data, but is less susceptible to the noise - depending on how exactly we parameterize \"noise\" we may throw out actual correlation, but if we balance it right we keep that signal and greatly improve generalizability.\n",
        "\n",
        "### Look carefully at the above plot and think of ways you can quantify the difference between the blue and green lines...\n"
      ]
    },
    {
      "metadata": {
        "id": "7aQlX9e9lQLr",
        "colab_type": "code",
        "outputId": "91f5f2af-ca05-42fb-c0c3-144feea505e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Now with regularization via ridge regression\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge_reg = Ridge().fit(X, y)\n",
        "mean_squared_error(y, ridge_reg.predict(X))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.895862166800143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "qiMXYAWGomcB",
        "colab_type": "code",
        "outputId": "358e7c75-6ce7-492f-9470-9096b7354684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# The score is a bit worse than OLS - but that's expected (we're adding bias)\n",
        "# Let's try split\n",
        "\n",
        "ridge_reg_split = Ridge().fit(X_train, y_train)\n",
        "mean_squared_error(y_test, ridge_reg_split.predict(X_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.192201358877668"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "PJhjFFeF2uoA",
        "colab_type": "code",
        "outputId": "eee52d90-1069-412d-aafe-d3b19d4a0716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4730
        }
      },
      "cell_type": "code",
      "source": [
        "# A little better (to same test split w/OLS) - can we improve it further?\n",
        "# We just went with defaults, but as always there's plenty of parameters\n",
        "help(Ridge)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Ridge in module sklearn.linear_model.ridge:\n",
            "\n",
            "class Ridge(_BaseRidge, sklearn.base.RegressorMixin)\n",
            " |  Linear least squares with l2 regularization.\n",
            " |  \n",
            " |  Minimizes the objective function::\n",
            " |  \n",
            " |  ||y - Xw||^2_2 + alpha * ||w||^2_2\n",
            " |  \n",
            " |  This model solves a regression model where the loss function is\n",
            " |  the linear least squares function and regularization is given by\n",
            " |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
            " |  This estimator has built-in support for multi-variate regression\n",
            " |  (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  alpha : {float, array-like}, shape (n_targets)\n",
            " |      Regularization strength; must be a positive float. Regularization\n",
            " |      improves the conditioning of the problem and reduces the variance of\n",
            " |      the estimates. Larger values specify stronger regularization.\n",
            " |      Alpha corresponds to ``C^-1`` in other linear models such as\n",
            " |      LogisticRegression or LinearSVC. If an array is passed, penalties are\n",
            " |      assumed to be specific to the targets. Hence they must correspond in\n",
            " |      number.\n",
            " |  \n",
            " |  fit_intercept : boolean\n",
            " |      Whether to calculate the intercept for this model. If set\n",
            " |      to false, no intercept will be used in calculations\n",
            " |      (e.g. data is expected to be already centered).\n",
            " |  \n",
            " |  normalize : boolean, optional, default False\n",
            " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
            " |      If True, the regressors X will be normalized before regression by\n",
            " |      subtracting the mean and dividing by the l2-norm.\n",
            " |      If you wish to standardize, please use\n",
            " |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
            " |      on an estimator with ``normalize=False``.\n",
            " |  \n",
            " |  copy_X : boolean, optional, default True\n",
            " |      If True, X will be copied; else, it may be overwritten.\n",
            " |  \n",
            " |  max_iter : int, optional\n",
            " |      Maximum number of iterations for conjugate gradient solver.\n",
            " |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
            " |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
            " |  \n",
            " |  tol : float\n",
            " |      Precision of the solution.\n",
            " |  \n",
            " |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n",
            " |      Solver to use in the computational routines:\n",
            " |  \n",
            " |      - 'auto' chooses the solver automatically based on the type of data.\n",
            " |  \n",
            " |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
            " |        coefficients. More stable for singular matrices than\n",
            " |        'cholesky'.\n",
            " |  \n",
            " |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
            " |        obtain a closed-form solution.\n",
            " |  \n",
            " |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
            " |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
            " |        more appropriate than 'cholesky' for large-scale data\n",
            " |        (possibility to set `tol` and `max_iter`).\n",
            " |  \n",
            " |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
            " |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
            " |        procedure.\n",
            " |  \n",
            " |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
            " |        its improved, unbiased version named SAGA. Both methods also use an\n",
            " |        iterative procedure, and are often faster than other solvers when\n",
            " |        both n_samples and n_features are large. Note that 'sag' and\n",
            " |        'saga' fast convergence is only guaranteed on features with\n",
            " |        approximately the same scale. You can preprocess the data with a\n",
            " |        scaler from sklearn.preprocessing.\n",
            " |  \n",
            " |      All last five solvers support both dense and sparse data. However,\n",
            " |      only 'sag' and 'saga' supports sparse input when `fit_intercept` is\n",
            " |      True.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         Stochastic Average Gradient descent solver.\n",
            " |      .. versionadded:: 0.19\n",
            " |         SAGA solver.\n",
            " |  \n",
            " |  random_state : int, RandomState instance or None, optional, default None\n",
            " |      The seed of the pseudo random number generator to use when shuffling\n",
            " |      the data.  If int, random_state is the seed used by the random number\n",
            " |      generator; If RandomState instance, random_state is the random number\n",
            " |      generator; If None, the random number generator is the RandomState\n",
            " |      instance used by `np.random`. Used when ``solver`` == 'sag'.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         *random_state* to support Stochastic Average Gradient.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
            " |      Weight vector(s).\n",
            " |  \n",
            " |  intercept_ : float | array, shape = (n_targets,)\n",
            " |      Independent term in decision function. Set to 0.0 if\n",
            " |      ``fit_intercept = False``.\n",
            " |  \n",
            " |  n_iter_ : array or None, shape (n_targets,)\n",
            " |      Actual number of iterations for each target. Available only for\n",
            " |      sag and lsqr solvers. Other solvers will return None.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |  \n",
            " |  See also\n",
            " |  --------\n",
            " |  RidgeClassifier : Ridge classifier\n",
            " |  RidgeCV : Ridge regression with built-in cross validation\n",
            " |  :class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n",
            " |      combines ridge regression with the kernel trick\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.linear_model import Ridge\n",
            " |  >>> import numpy as np\n",
            " |  >>> n_samples, n_features = 10, 5\n",
            " |  >>> np.random.seed(0)\n",
            " |  >>> y = np.random.randn(n_samples)\n",
            " |  >>> X = np.random.randn(n_samples, n_features)\n",
            " |  >>> clf = Ridge(alpha=1.0)\n",
            " |  >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE\n",
            " |  Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            " |        normalize=False, random_state=None, solver='auto', tol=0.001)\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Ridge\n",
            " |      _BaseRidge\n",
            " |      abc.NewBase\n",
            " |      sklearn.linear_model.base.LinearModel\n",
            " |      abc.NewBase\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      sklearn.base.RegressorMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit Ridge regression model\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
            " |          Training data\n",
            " |      \n",
            " |      y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
            " |          Target values\n",
            " |      \n",
            " |      sample_weight : float or numpy array of shape [n_samples]\n",
            " |          Individual weights for each sample\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : returns an instance of self.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict using the linear model\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
            " |          Samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array, shape (n_samples,)\n",
            " |          Returns predicted values.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : boolean, optional\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.RegressorMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Returns the coefficient of determination R^2 of the prediction.\n",
            " |      \n",
            " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
            " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
            " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
            " |      The best possible score is 1.0 and it can be negative (because the\n",
            " |      model can be arbitrarily worse). A constant model that always\n",
            " |      predicts the expected value of y, disregarding the input features,\n",
            " |      would get a R^2 score of 0.0.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like, shape = (n_samples, n_features)\n",
            " |          Test samples. For some estimators this may be a\n",
            " |          precomputed kernel matrix instead, shape = (n_samples,\n",
            " |          n_samples_fitted], where n_samples_fitted is the number of\n",
            " |          samples used in the fitting for the estimator.\n",
            " |      \n",
            " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
            " |          True values for X.\n",
            " |      \n",
            " |      sample_weight : array-like, shape = [n_samples], optional\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          R^2 of self.predict(X) wrt. y.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F4eY9TKw4S4F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How to tune alpha? For now, let's loop and try values.\n",
        "\n",
        "(For longterm/stretch/next week, check out [cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV).)"
      ]
    },
    {
      "metadata": {
        "id": "DISx148Z4Sqi",
        "colab_type": "code",
        "outputId": "1a0e093a-23e2-4426-e4c2-2775ab50210f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3532
        }
      },
      "cell_type": "code",
      "source": [
        "alphas = []\n",
        "mses = []\n",
        "\n",
        "for alpha in range(0, 200, 1):\n",
        "  ridge_reg_split = Ridge(alpha=alpha).fit(X_train, y_train)\n",
        "  mse = mean_squared_error(y_test, ridge_reg_split.predict(X_test))\n",
        "  print(alpha, mse)\n",
        "  alphas.append(alpha)\n",
        "  mses.append(mse)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 26.273991426429053\n",
            "1 26.192201358877668\n",
            "2 26.118328007697226\n",
            "3 26.051117952293595\n",
            "4 25.989569283205444\n",
            "5 25.93287356811407\n",
            "6 25.880372753122625\n",
            "7 25.831526788692837\n",
            "8 25.785889053385123\n",
            "9 25.743087513207442\n",
            "10 25.702810145277628\n",
            "11 25.66479356379559\n",
            "12 25.628814073392263\n",
            "13 25.59468057863174\n",
            "14 25.56222892458644\n",
            "15 25.53131734932431\n",
            "16 25.50182280665663\n",
            "17 25.473637974726547\n",
            "18 25.44666880864133\n",
            "19 25.420832527348388\n",
            "20 25.396055949160893\n",
            "21 25.372274108781504\n",
            "22 25.349429102822004\n",
            "23 25.327469121742823\n",
            "24 25.30634763462536\n",
            "25 25.286022699825878\n",
            "26 25.266456379775086\n",
            "27 25.24761424230921\n",
            "28 25.229464934192976\n",
            "29 25.211979815108453\n",
            "30 25.19513264248022\n",
            "31 25.178899299197408\n",
            "32 25.163257557659424\n",
            "33 25.14818687468406\n",
            "34 25.13366821272317\n",
            "35 25.11968388357409\n",
            "36 25.106217411385522\n",
            "37 25.093253412260974\n",
            "38 25.080777488180427\n",
            "39 25.068776133307583\n",
            "40 25.057236651039766\n",
            "41 25.046147080399017\n",
            "42 25.035496130566347\n",
            "43 25.02527312253186\n",
            "44 25.015467936977434\n",
            "45 25.006070967630855\n",
            "46 24.997073079433843\n",
            "47 24.98846557095439\n",
            "48 24.980240140548986\n",
            "49 24.972388855844812\n",
            "50 24.96490412616673\n",
            "51 24.95777867758141\n",
            "52 24.951005530271846\n",
            "53 24.944577977990345\n",
            "54 24.9384895693689\n",
            "55 24.9327340908919\n",
            "56 24.92730555135946\n",
            "57 24.92219816768905\n",
            "58 24.917406351921144\n",
            "59 24.912924699309215\n",
            "60 24.9087479773882\n",
            "61 24.904871115926827\n",
            "62 24.901289197679855\n",
            "63 24.897997449864803\n",
            "64 24.8949912362963\n",
            "65 24.892266050117623\n",
            "66 24.889817507075655\n",
            "67 24.88764133929069\n",
            "68 24.885733389477625\n",
            "69 24.88408960557926\n",
            "70 24.882706035776263\n",
            "71 24.88157882384208\n",
            "72 24.880704204813682\n",
            "73 24.880078500952195\n",
            "74 24.879698117969724\n",
            "75 24.879559541500758\n",
            "76 24.87965933379892\n",
            "77 24.879994130641087\n",
            "78 24.8805606384229\n",
            "79 24.881355631430836\n",
            "80 24.882375949277577\n",
            "81 24.88361849448833\n",
            "82 24.88508023022692\n",
            "83 24.886758178151386\n",
            "84 24.888649416389928\n",
            "85 24.89075107762813\n",
            "86 24.89306034730016\n",
            "87 24.895574461876226\n",
            "88 24.898290707239912\n",
            "89 24.90120641714914\n",
            "90 24.90431897177517\n",
            "91 24.907625796314402\n",
            "92 24.911124359668285\n",
            "93 24.914812173186736\n",
            "94 24.918686789471128\n",
            "95 24.92274580123304\n",
            "96 24.92698684020521\n",
            "97 24.93140757610152\n",
            "98 24.93600571562298\n",
            "99 24.94077900150688\n",
            "100 24.945725211616683\n",
            "101 24.950842158070053\n",
            "102 24.95612768640294\n",
            "103 24.96157967476758\n",
            "104 24.96719603316249\n",
            "105 24.972974702692664\n",
            "106 24.97891365485829\n",
            "107 24.985010890870456\n",
            "108 24.99126444099231\n",
            "109 24.99767236390434\n",
            "110 25.004232746092597\n",
            "111 25.010943701258537\n",
            "112 25.017803369749362\n",
            "113 25.02480991800798\n",
            "114 25.0319615380414\n",
            "115 25.03925644690685\n",
            "116 25.04669288621445\n",
            "117 25.05426912164612\n",
            "118 25.061983442489353\n",
            "119 25.069834161185728\n",
            "120 25.077819612893087\n",
            "121 25.085938155060855\n",
            "122 25.09418816701804\n",
            "123 25.102568049573183\n",
            "124 25.111076224625787\n",
            "125 25.119711134788766\n",
            "126 25.128471243021377\n",
            "127 25.137355032272303\n",
            "128 25.14636100513223\n",
            "129 25.155487683495902\n",
            "130 25.164733608232837\n",
            "131 25.174097338866744\n",
            "132 25.183577453263027\n",
            "133 25.193172547324206\n",
            "134 25.2028812346929\n",
            "135 25.212702146462046\n",
            "136 25.222633930892243\n",
            "137 25.232675253135735\n",
            "138 25.24282479496694\n",
            "139 25.25308125451928\n",
            "140 25.26344334602802\n",
            "141 25.273909799578966\n",
            "142 25.284479360862818\n",
            "143 25.29515079093497\n",
            "144 25.305922865980495\n",
            "145 25.31679437708437\n",
            "146 25.32776413000649\n",
            "147 25.338830944961526\n",
            "148 25.349993656403374\n",
            "149 25.361251112814\n",
            "150 25.37260217649681\n",
            "151 25.384045723373994\n",
            "152 25.39558064278813\n",
            "153 25.407205837307533\n",
            "154 25.418920222535693\n",
            "155 25.430722726924202\n",
            "156 25.442612291589445\n",
            "157 25.45458787013271\n",
            "158 25.466648428463827\n",
            "159 25.478792944627976\n",
            "160 25.491020408635883\n",
            "161 25.50332982229701\n",
            "162 25.515720199055906\n",
            "163 25.528190563831558\n",
            "164 25.540739952859465\n",
            "165 25.5533674135368\n",
            "166 25.56607200427009\n",
            "167 25.578852794325684\n",
            "168 25.591708863682904\n",
            "169 25.604639302889613\n",
            "170 25.617643212920317\n",
            "171 25.63071970503678\n",
            "172 25.643867900650903\n",
            "173 25.657086931189966\n",
            "174 25.670375937964163\n",
            "175 25.68373407203625\n",
            "176 25.697160494093474\n",
            "177 25.71065437432154\n",
            "178 25.724214892280617\n",
            "179 25.737841236783435\n",
            "180 25.751532605775324\n",
            "181 25.765288206216116\n",
            "182 25.779107253964067\n",
            "183 25.792988973661497\n",
            "184 25.8069325986223\n",
            "185 25.820937370721257\n",
            "186 25.83500254028498\n",
            "187 25.849127365984653\n",
            "188 25.863311114730404\n",
            "189 25.87755306156723\n",
            "190 25.891852489572667\n",
            "191 25.906208689755893\n",
            "192 25.920620960958406\n",
            "193 25.93508860975623\n",
            "194 25.949610950363557\n",
            "195 25.964187304537848\n",
            "196 25.97881700148633\n",
            "197 25.99349937777395\n",
            "198 26.008233777232597\n",
            "199 26.023019550871716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iRB3KHyWiO4y",
        "colab_type": "code",
        "outputId": "fc530fa0-ff70-41e0-9229-6edc9595637e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import scatter\n",
        "scatter(alphas, mses);"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlYwDv5lZyTjwm5mVjAO/mVnJ/H8a3r1B+t/WuAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "WzgTBd-FcctM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What's the intuition? What are we doing?\n",
        "\n",
        "The `alpha` parameter corresponds to the weight being given to the extra penalty being calculated by [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization) (this parameter is sometimes referred to as $\\lambda$ in the context of ridge regression).\n",
        "\n",
        "Normal linear regression (OLS) minimizes the **sum of square error of the residuals**.\n",
        "\n",
        "Ridge regression minimizes the **sum of square error of the residuals** *AND* **the squared slope of the fit model, times the alpha parameter**.\n",
        "\n",
        "This is why the MSE for the first model in the for loop (`alpha=0`) is the same as the MSE for linear regression - it's the same model!\n",
        "\n",
        "As `alpha` is increased, we give more and more penalty to a steep slope. In two or three dimensions this is fairly easy to visualize - beyond, think of it as penalizing coefficient size. Each coefficient represents the slope of an individual dimension (feature) of the model, so ridge regression is just squaring and summing those.\n",
        "\n",
        "So while `alpha=0` reduces to OLS, as `alpha` approaches infinity eventually the penalty gets so extreme that the model will always output every coefficient as 0 (any non-zero coefficient resulting in a penalty that outweighs whatever improvement in the residuals), and just fit a flat model with intercept at the mean of the dependent variable.\n",
        "\n",
        "Of course, what we want is somewhere in-between these extremes. Intuitively, what we want to do is apply an appropriate \"cost\" or penalty to the model for fitting parameters, much like adjusted $R^2$ takes into account the cost of adding complexity to a model. What exactly is an appropriate penalty will vary, so you'll have to put on your model comparison hat and give it a go!\n",
        "\n",
        "PS - scaling the data helps, as that way this cost is consistent and can be added uniformly across features, and it is simpler to search for the `alpha` parameter.\n",
        "\n",
        "### Bonus - magic! âœ¨\n",
        "\n",
        "Ridge regression doesn't just reduce overfitting and help with the third aspect of well-posed problems (poor generalizability). It can also fix the first two (no unique solution)!"
      ]
    },
    {
      "metadata": {
        "id": "rdogs9EMX6Vd",
        "colab_type": "code",
        "outputId": "fa3a8315-249d-4189-cd88-c726b30c28a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "df_tiny = df.sample(10, random_state=27)\n",
        "print(df_tiny.shape)\n",
        "X = df_tiny.drop('Price', axis='columns')\n",
        "y = df_tiny.Price\n",
        "\n",
        "lin_reg = LinearRegression().fit(X, y)\n",
        "lin_reg.score(X, y)  # Perfect multi-collinearity!\n",
        "# NOTE - True OLS would ðŸ’¥ here\n",
        "# scikit protects us from actual error, but still gives a poor model"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "zesVR59NhA7A",
        "colab_type": "code",
        "outputId": "cda8bb10-1107-4872-bcc9-bfa270b28236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ridge_reg = Ridge().fit(X, y)\n",
        "ridge_reg.score(X, y)  # More plausible (not \"perfect\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9760119331942763"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "WP6zwLtshaVR",
        "colab_type": "code",
        "outputId": "2a3b59a6-acb2-4355-f9cd-faeb551f6282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Using our earlier test split\n",
        "mean_squared_error(y_test, lin_reg.predict(X_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "103.04429449784261"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "QeL_O8vNhSqj",
        "colab_type": "code",
        "outputId": "ec18448b-2ea4-4f25-eb19-1816d62d3ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Ridge generalizes *way* better (and we've not even tuned alpha)\n",
        "mean_squared_error(y_test, ridge_reg.predict(X_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41.79869373639458"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "17HQAHuX_NfS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# e.g. (x1^2 + x2^2 + ...) * alpha is the extra penalty from Ridge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2N5WDV6nd3S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## And a bit more math\n",
        "\n",
        "The regularization used by Ridge Regression is also known as **$L^2$ regularization**, due to the squaring of the slopes being summed. This corresponds to [$L^2$ space](https://en.wikipedia.org/wiki/Square-integrable_function), a metric space of square-integrable functions that generally measure what we intuitively think of as \"distance\" (at least, on a plane) - what is referred to as Euclidean distance.\n",
        "\n",
        "The other famous norm is $L^1$, also known as [taxicab geometry](https://en.wikipedia.org/wiki/Taxicab_geometry), because it follows the \"grid\" to measure distance like a car driving around city blocks (rather than going directly like $L^2$). When referred to as a distance this is called \"Manhattan distance\", and can be used for regularization (see [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics%29), which [uses the $L^1$ norm](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when)).\n",
        "\n",
        "All this comes down to - regularization means increasing model bias by \"watering down\" coefficients with a penalty typically based on some sort of distance metric, and thus reducing variance (overfitting the model to the noise in the data). It gives us another lever to try and another tool for our toolchest!\n",
        "\n",
        "## Putting it all together - one last example\n",
        "\n",
        "The official scikit-learn documentation has many excellent examples - [this one](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py) illustrates how ridge regression effectively reduces the variance, again by increasing the bias, penalizing coefficients to reduce the effectiveness of features (but also the impact of noise).\n",
        "\n",
        "```\n",
        "Due to the few points in each dimension and the straight line that linear regression uses to follow these points as well as it can, noise on the observations will cause great variance as shown in the first plot. Every lineâ€™s slope can vary quite a bit for each prediction due to the noise induced in the observations.\n",
        "\n",
        "Ridge regression is basically minimizing a penalised version of the least-squared function. The penalising shrinks the value of the regression coefficients. Despite the few data points in each dimension, the slope of the prediction is much more stable and the variance in the line itself is greatly reduced, in comparison to that of the standard linear regression\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "LaOYdswIB6Bo",
        "colab_type": "code",
        "outputId": "b9150b9d-67b1-4c88-b3a7-97b669db3ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "cell_type": "code",
      "source": [
        "# Code source: GaÃ«l Varoquaux\n",
        "# Modified for documentation by Jaques Grobler\n",
        "# License: BSD 3 clause\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import linear_model\n",
        "\n",
        "X_train = np.c_[.5, 1].T\n",
        "y_train = [.5, 1]\n",
        "X_test = np.c_[0, 2].T\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "classifiers = dict(ols=linear_model.LinearRegression(),\n",
        "                   ridge=linear_model.Ridge(alpha=.1))\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    fig, ax = plt.subplots(figsize=(4, 3))\n",
        "\n",
        "    for _ in range(6):\n",
        "        this_X = .1 * np.random.normal(size=(2, 1)) + X_train\n",
        "        clf.fit(this_X, y_train)\n",
        "\n",
        "        ax.plot(X_test, clf.predict(X_test), color='gray')\n",
        "        ax.scatter(this_X, y_train, s=3, c='gray', marker='o', zorder=10)\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    ax.plot(X_test, clf.predict(X_test), linewidth=2, color='blue')\n",
        "    ax.scatter(X_train, y_train, s=30, c='red', marker='+', zorder=10)\n",
        "\n",
        "    ax.set_title(name)\n",
        "    ax.set_xlim(0, 2)\n",
        "    ax.set_ylim((0, 1.6))\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('y')\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAADQCAYAAAA+nmWYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyfkeDBNcHuYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAADQCAYAAAA+nmWYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XlYXfd54PHvyy60IKEFJCTQhpBA\n2wUESEgIxVqwLctukk6dNm7cpnGbNm3TmWem05lO20k782TaafpMJmkTx3GcJpPdsevYlixZixG6\n93KBe0EsAkkggYRWhNiEWO9v/jiXayQLC0lcLqD38zw83LP8Dr8jpFfnvL9zfq8YY1BKqfEWEuwO\nKKUeTxp8lFJBocFHKRUUGnyUUkGhwUcpFRQafJRSQaHBRwWMiFSLSP4I2/JF5OI4d0lNIGHB7oCa\nuowxacHug5q49MpHBYSI6H9s6mNp8FFjRkTOi8ifi8hJ4JaIXBSRnb5t00TkNRG5KSI1wKa72qaL\niEdEOkXk5yLyUxH5u2Hb94pIuYi0iYhdRNaP79mpsabBR421zwBPA7OBgWHr/xpY4fvaA3xuaIOI\nRABvAK8BscCPgV8btt0GvAr8PjAX+DbwlohEBvA8VIBp8FFj7evGmAvGmNt3rf93wP8wxrQaYy4A\nXx+2LQcr//h1Y0y/MeaXgGvY9peAbxtjio0xg8aY7wO9vnZqktLgo8bahRHWL7prW+Nd25rNnW85\nD983CfgPvluuNhFpA5b42qlJSoOPGmsjTZNwGStgDEm8a1uCiMiwdcP3vYB11TR72Fe0MebHY9Nl\nFQwafNR4+RnwFyIyR0QWA388bJsDGAS+JCJhIvIskDVs+3eAPxCRbLFMF5GnRWTm+HVfjTUNPmq8\n/HesW61zwEHgB0MbjDF9wCeBzwNtwGeBt7HyOhhjSoEvAN8AbgJngRfHr+sqEEQnE1MTkYgUA98y\nxnwv2H1RgaFXPmpCEJHtIhLvu+36HLAeOBDsfqnA0adQ1USRgpUXmg40AJ82xlwObpdUIOltl1Iq\nKPS2SykVFFPqtmvevHlm6dKlwe6GUo+NsrKyFmPM/IdpO6WCz9KlSyktLQ12N5R6bIhI4/33uje9\n7VJKBYUGH6VUUGjwUUoFhQYfpVRQBCz4iMirInJNRKpG2J4vIu2+2enKReSvhm0rEJE6ETkrIv85\nUH1USgVPIK98XgMK7rPPcWPMRt/XVwBEJBT4JvAkkAp8RkRSA9hPpVQQBCz4GGMKgdaHaJoFnDXG\nNPjedv4J8OyYdk4pFXTBzvlsFpEKEdkvIkNlVhK4cxa7i751SqkpJJgPGbqBJGNMl4g8BbwJJD/o\nQUTkJaw5fklMTLzP3kqpiSJoVz7GmA5jTJfv87tAuIjMA5q5cwrNxb51Ix3nZWNMpjEmc/78h3rK\nWykVBEELPr65W8T3OcvXlxtACZAsIst8JVWeB94KVj+VUvfm9XofqX3AbrtE5MdAPjDPV5P7r4Fw\nAGPMt4BPA18UkQHgNvC8r3rBgIh8CXgPCAVeNcZUB6qfSqkH09/fT3l5OW+9VfNIx5lS8/lkZmYa\nfbFUqcDo7u6mpKSE119v5PDhDE6dWoMxoWXGmMyHOd6UeqtdKTX22trasNsd/PznbRw7lsP589sB\nCA839Pc//HE1+Cil7uny5csUFjr4+c+hqCiXq1fjAJgxw8uWLZWsXXuYr33t4Y+vwUcp5WeMoaGh\ngaNHXbz++myczk/Q1jYbgAULBsjOdpKWVkRUVO8j/ywNPkopBgcHqamp4cCBMt5+exmlpc/R3T0N\ngKVLe8jIOMqaNWWEhQ0iIqSmprFnzx7+5m/+5qF/pgYfpR5jfX19uN1u3n67hoMH11Fe/ln6+62w\nkJragc12kBUrqgkJgdDQUHJycsnLyyMiIoLa2tpH+tkafJR6DHV1deFyuXjzzSaOHMmkpuZFjLEe\n+8vMvML69QdYsqQREYiKimLnzp3YbDYGBwc5evQoHo+Hnp6eR+qDBh+lHiM3btzAbnfw+uvtHD+e\nQ0PDJwAICzPk5NSzfv17LFhwHYCYmBj27dvH8uXLuXTpEj/4wQ9obGzEGIOIsGLFikfqiwYfpR4D\nFy9e5PhxB6+/LtjtuVy+vBCA6dMNmzdXsG7dEWJiOgFYuHAhn/rUp5g1axZlZWW8+eabdHZa26ZN\nm0ZmZiZ5eXmEhYXxwgsvPHSfNPgoNUUZYzhz5gxHjjh56615OJ27uHnTGrmaN2+QrCwH69YVMW1a\nLyLCqlUpPPvss7S3t3P48GHq6ur8r1DExcWxc+dOVq5cOWb90+Cj1BQzMDBAZWUlhw552L9/OSUl\nv86tW9bIVWJiD5mZx1izppTw8EFCQ0PJzMxm27Zt1NTU8J3vfIebN28CEBISwtq1a9m5cycxMTFj\n3k8NPkpNET09PZSVlbF//ykOHVqPx/MC/f3hAKxe3Y7Ndojk5BpCQgwRERHs2LGThQsXUlpayte+\n9jX/VU50dDRbtmxh06ZNREREBKy/GnyUmuQ6OjooLi7mrbcucOzYJmpqfhev1xq5stkus3HjQRIT\nzyMCM2bMYPfu3XR2duJyufxXOQDx8fHk5eWRkpJCSEjgJ7zQ4KPUJHXt2jXsdgdvvtlBUdEW6ut3\nAdbIVXb2WTZuPERc3DUA5s2bR3Z2Ng0NDbzxxhsMvVBuPTCYypYtW1i0aNG49l+Dj1KTiDGGpqYm\njh938G//FordvpVLl6yRq+hoL9nZJ9mw4SizZ3cAsHjxYhITE6mqquKdd94hJCQEY6zbrk2bNpGV\nlcWsWbOCci6BnM/nVWAvcM0Ys/Ye238L+HNAgE7gi8aYCt+28751g8DAw76yr9RU4fV6qaur4+hR\nJ++8swCns4DWVmvkKjZ2gKwsB+vX24mO7kFESExMIjQ0lIaGBi5evEhYmPVPPSYmhpycHDZu3BjQ\nfM5oBPLK5zXgG8C/jrD9HLDdGHNTRJ4EXgayh23fYYxpCWD/lJrw+vv7qaio4P33PRw8uJKSkt+g\nqysagIQEa+Rq7doywsMHCA0NZeHCxbS2ttLY2EhERAQhISF4vV4SEhLIyclh1apV45LPGY2ABR9j\nTKGILP2Y7fZhi06suZqVUsDt27cpKSnhwIFTHDmyAY/nRfr6rJGr5OQ20tPfJyXFGrkKDw9n1qy5\n3Lhxg+bmZmbMmAFYgWvt2rXk5OSMez5nNCZKzufzwP5hywY4KCIG+LYx5uXgdEup8dXW1obD4WD/\n/oscO5ZNdfUX/CNX69dfwmY7xNKl1shVREQEXq+X/v5+ent7iYmJob29nf7+fnJzc4OazxmNoAcf\nEdmBFXy2Dlu91RjTLCILgEMiUusrQniv9lo6R016V65c4cQJO2+/3UVR0RbOnn0SgNBQQ1bWGdLT\nDxMffxWAsLAwBgYG8Hq9zJ8/n46ODrq6uoiNjeXJJ5+cEPmc0Qhq8BGR9cArwJPGmBtD640xzb7v\n10TkDawqpvcMPr6ropfBmsM54J1WaowYYzh37hzHj9t5550IHI6tXLxo3R5Nm+YlM7OCjIwPmD27\n/Y52sbGxREdH09zczOXLl0lKSmLv3r0TKp8zGkELPiKSCPwSeMEYc3rY+ulAiDGm0/d5N/CVIHVT\nqTHn9Xqprq7m2LFiDh6Mx+l8mpaWOQDMnt1PZqaD9HQn0dG3/W0iIyNZtmwZvb29nD9/HhEhLS1t\nwuZzRiOYpXP+CpgL/LOvfNfQkHoc8IZvXRjwI2PMgUD1U6nx0tfXh8fj4cgRD++/v4qSkt+ks9Ma\nuYqP7yYz8wPWr3cTETHgb5OYmEhcXBzNzc3U1tYSFRXFli1bJnw+ZzS0dI5SAXbr1i1cLheHDtVy\n9OhGPJ5Menutkatly1rJyDjCmjU1hIZa/xYjIiKw2WyEhYVRVVVFe3s7sbGxZGdnT7h8joho6Ryl\nJprW1lbsdjsHD16isDCLqqrfZ3DQysmkpl4kM/MIy5adw7rIh0WLFmGz2bh+/Toej4e+vj6SkpIo\nKCiYdPmc0dDgo9QYa25upqjoBAcOdGO353L69F4AQkIMNlstWVkfsHDhFd+6EDIzM1m6dClVVVW8\n++67UyKfMxoafJQaA0MTdxUVOXjvvSgcjm1cuGAFjsjIQWw2N9nZdubMaQM+fLtcRCguLsblck2p\nfM5oaPBR6hEMDg5SWVnJBx8Uc/jwIpzOZ7h+PRaAmTN7ychwsmlTMdOnWyNXixcv5umnn+bcuXMc\nPnzYn8+ZTM/njBUNPko9hN7eXsrKyjh61MPRoymUlHyWjo7pAMyd20FWVhE2WzkREf2ICBs32sjN\nzaW0tJTvfe97Uz6fMxoafJR6AJ2dnTidTo4cOc0HH9jweF6ip8cauVq06CqbNxeRmlpNaKj1ztXO\nnU8SHx9PcXEx3/zmNx+bfM5oaPBRahSuX7+O3W7n/fcvUVS0mcrKJ/wjV8uXN5Cbe4LlyxsQgZkz\nZ7J37176+/txOp3s37//scvnjIYGH6VGYIzhwoULFBWd4P33b2O3b6Ou7lkARLykpVWRm2tn0aLL\ngFVyZs+ePVy6dIl33333sc7njIYGH6XuMjRxV1GRncOHp+Nw5NHUlABAWFg/Nls5mzc7iI215j9e\nsWIFeXl5nDp1ih/96EeazxklDT5K+QwMDFBRUUFhYTHHji3G6fw1rl2zRq6iom6TleUiO9vF9Ond\nAGzYsIF169bhdrt57bXXNJ/zgDT4qMfe7du3KS0t5dgxD4WFa3C5PucfuYqJaWPzZic2m5vIyH7C\nwsLIytpCfHw8LpeLH/7wh5rPeUgafNRjq729HYfDwbFjpzl+PAO3+w/o6bHyMnFxV8jNtZOWVk1o\nqJdp06axefM2QkNDcblc2O12zec8Ig0+6rFz5coV7HY7R45cxm7fwsmTuxgcDAVg6dJz5OaeYOXK\nekSsCde3bNlCa2srRUVFms8ZQxp81GNhaOIuu93O0aO92O1bqa39JAAihtTUanJz7SQkXAKs2uSZ\nmZmcO3eOAwcOaD4nAAIafEZRPkeA/wM8BXQDLxpj3L5tnwP+0rfr3xljvh/Ivqqpyev1UlNTw/Hj\nJzh+fBYnTmynqWkJAGFhA2zcWM7mzXbmzrVGrpKSkli9ejXV1dW88847ms8JoEBf+bzGx5fPeRJI\n9n1lA/8CZItILNbkY5lYk8mXichbxpibIxxHTVb5+db3Y8fG9LB9fX2Ul5fzwQcOTpxYit3+Sa5f\nnw9AVFQPmzZZI1czZtwCICUlhfj4eMrLy3nvvfc0nzMOAhp87lc+B3gW+FdjzWjmFJHZIrIQawbE\nQ8aYVgAROQQUAD8OZH/V5Hfr1i2cTieFheU4HOtwOn+Hzk7riiUmpoOcHAfp6W4iI/sIDQ0lLW09\nkZGRVFRUUFdXp/mccRTsnE8CcGHY8kXfupHWq0mkrKyM999/H4CdO3eSkZHh39aZkUFbeztL6uvv\nWC75+7+nsbGRvLw8MjIyKCsro7Cw0L88ktbWVo4ePYrT2YjDkUVp6R/R2xsFwIIF18jNPcHatVWE\nhnqJiIhg3boMuru7qays1HxOkAQ7+DwyLZ0zcRUWFtLT0+P/PDx4tLW3Mzgw8JHlqqoqjDH+/QsL\nC+no6PhI+yGNjY0cPHiQkyd7sdu3UFHxLIOD1l/rpKTz5OaeIDn5LCLWHDopKSlcuXKFsrIyzecE\nWbCDTzOwZNjyYt+6Zqxbr+Hrj93rAFo6Z+LKy8vzX/nk5eXdse3aT39KYWEhv/fDHzJz5kyu/eM/\nUlhYyNqkJP+Vz1C7oSufIcYYSkpKKCoqoqZmFidObKWubjXGCCKGNWtq2LrVQULCRcAqNbN06VLq\n6+spKyvTfM4EEfAJ5H05n7dHGO16GvgS1mhXNvB1Y0yWL+FcBqT7dnUDGUM5oJHoBPKT0AMknG/d\nusWhQ4eorKymtnYZJ05sobFxKQChoQNs2FBBbq6DuXOtEnBxcXHMnz+f06dP+5/PmWj1yie7CTuB\n/CjK57yLFXjOYg21/45vW6uI/C1Q4jvUV+4XeNQkdZ+gY4yhvr6eI0eOcOHCVaqq1mK3f4Fr1xYA\n1shVZmYJ2dnFzJxpjVwtXryYqKgo6uvruX79uuZzJigtnaMmpKFyMyUlJbS1DVJWlk5x8Wba263c\nzMyZHWze7CQjo4zIyD5EhMTERPr6+rh8+TJRUVFkZGRoPifAJuyVj1IPwuv10tDQgNPppKGhgc7O\naIqLN1NWlkV3dyQA8+dfY+tWB2lpJwkL8xIWFsbixUtpbW2lsbFR8zmTiAYfFXTt7e14PB5KS0u5\ndesWN27EYrc/zcmTG+nvt965SkxsZNs2BytW1BESAlFRUcTFxXH58mXOnz9PUlISTz31FMnJyZrP\nmSQ0+KigGBwcpK6uDrfbTb3vWZ+LFxfhcOylpiYFY6xKeqtXn2LbtmISEhoBa7g8JiaGS5cuceHC\nBc3nTGIafNS4amlpwe12U1FRQXd3NyIhnDmzEqczj/p666kLa+TqJLm5TubOvQ5Yb5eHh4fT0tLC\nwMCAPp8zBWjwUQHX399PdXU1Ho+HpqYmRARjwqioWIfLtZ3m5rkAREb2kJVVRlZWMTNndgIwZ84c\n+vv7dT7kKUiDjwqYS5cu4Xa7qaqqore3l6ioKAYGoigp2UBJyTZaW63ZAmfM6GTrVhc2WymRkdYT\n0bGxsXR1dXHz5k2SkpLYu3ev5nOmGA0+akz19PRQWVmJ2+3mypUrhIaGMmvWLFpbwzhyJIuysmxu\n3bJGrubNu8727SWsWeMmLGzQt+8c2traaGtr03zOFKfBRz0yYwyNjY14PB5qamoYGBggNjaW+fPn\nU1c3wK9+lUt5+Ub6+qyRqyVLmsjPd7FsWTUhIRAWFkZ09Cw6Ojq4ffu25nMeExp81EPr6uqivLwc\nj8dDa2srkZGRJCYm0tHRQUVFOMXFeVRVpeD1WiNXKSmnyc8vZuHCBgAiIiIIDQ3l9u3bhIWFaT7n\nMaPBRz0Qr9dLfX09breb06dP4/V6WbJkCYsXL6axsYmDBwWXax+nT1sjVyEhg2RkVLN1q5M5c6zi\nelbuZ8D/vtXmzZs1n/MY0uCjRqWtrQ2Px4PH46Gzs5Po6Gj/FBcVFdW8804MLtdvcfHiPAAiInrJ\nyakgO9vB9OltgBV0ent76evr03yO0uCjRjYwMOB/ELChwbpVWrlyJVu3buXq1asUF1dRUrIOl+uL\n3LgxA7BGrrZvL2f9egeRkbcBK+gMzeuj+Rw15L7BR0T+GPihzp/8+Lh27Roej4eKigpu375NTEwM\n+fn5LFy4kJMnT/L664UUF2+irOxP6eqyRq7mzr3BE0+4SU4uJjx8EBEhLCyc/v5+oqOj2bFjh+Zz\n1B1Gc+UTB5SIiBt4FXjPTKVX4RVgTbheXV2N2+3m4sWLhISEsHr1amw2G16vF4fDwS9/WY7LtRW3\n+zn6+qy/OkuWNPPEE2UkJnoICYGQkBBEQhkcHGTRokWaz1Ejum/wMcb8pYj8N2A31nw73xCRnwHf\nNcbUB7qDKnCMMXc8CNjX18e8efPYvXs3aWlpnDt3jkOHDlFeHoLLtZ2KilUYYwWR1NR6tm93sWDB\naUQgNNQKONa2VM3nqPsaVc7HGGNE5ApwBRgA5gC/EJFDxpj/NFI7ESnAqssVCrxijPnqXdv/Cdjh\nW4wGFhhjZvu2DQKVvm1Nxph9oz8t9XFu377NyZMncbvdXLt2jfDwcNLS0khPT2f+/Pl4PB5eeeW7\nlJfPw+V6mro6a27skJBBNm06RW6ug5gYa37/oaATHh5OTk6O5nPUqN13MjER+VPgt4EW4BXgTWNM\nv4iEAGeMMStGaBcKnAZ2YVWfKAE+Y4ypGWH/PwZsxpjf9S13GWNmPMjJ6GRiIzPGcP78ef+DgEO3\nRenp6axdu5b+/n6Ki4txOkvxeFbgcm3nwgWrzlVERC/bttWSkXGc6GhritKQkBC8Xi+xsbFkZ2dr\nPucxFejJxGKBTxpjGoevNMZ4RWTvx7TLAs4aYxp8nfwJVp2uewYf4DNY06yqMdTZ2el/EPDmzZv+\nGf5sNhvx8fG0tLTw3nvvUVpaQ1nZelyuL9LSYl25TJ9+i507q0lLKyQiwpqi1Hop1LBkyRLN56hH\nMpqcz4gBwRhz6mOa3qv2Vva9dhSRJGAZcGTY6igRKcW6zfuqMebNEdpq6Zy7eL1ezpw5g8fj4fTp\n0xhjWLp0Kfn5+axZs4bw8HAuXLjAT37yEzyeJkpLs3G5vkxXl1Xnat68m+zZU8nSpYWEhw/6jxsS\nEqLP56gxM1Ge83ke+IUxZnDYuiRjTLOILAeOiEjlvRLcWjrnQ62trXg8HsrLy+nq6mLGjBls2bIF\nm83G3LlzMcZQV1eH3W7n5MkOSkq2Ulr6Kfr6wgFISrrKrl0eFi1yERLy4R9lZGQkmZmZms9RYyqQ\nwWekmlz38jzwR8NXGGOafd8bROQYYAN0dO0uAwMDnDp1Co/Hw7lz5xARkpOTsdlsJCcnExoaysDA\nAG63G7vdTlVVmG/kKgWv17pdWrfuIvn5xcTGViHy4bHnzJlDTk6O5nNUQAQy+JQAySKyDCvoPA/8\n5t07ichqrNEzx7B1c4BuY0yviMwDcoG/D2BfJ52rV6/idrs5efIkPT09zJ492/8g39DVSU9PDw6H\nA6ezmMrKebhcz1BbmwRYI1e5uQ3k5BQxc+a5O46t71up8RCw4GOMGRCRLwHvYQ21v2qMqRaRrwCl\nxpi3fLs+D/zkrgcX1wDfFhEvEIKV8xkpUf3Y6O3tpaqqCo/HQ3NzM6GhoaxZswabzcayZcsQ32VL\ne3s7TqeT0lIP5eUrcLleoKnJqnMVEdHHE080sGHDUaKirvmPLSKsXbtW8zlq3GjdrgnOGENzc7P/\nQcD+/n7mz59Peno669evJzo62r/v1atXsdvteDy1uN3rKS7eSktLDAAzZtymoOA0ycmHiIy85W8T\nERHBpk2bNJ+jHorW7ZqCuru7/Q8CXr9+nfDwcNauXUt6ejoJCQn+q5yh53esJPJFysqyKS7+Ml1d\n0wCYP7+Tp56qYfHi9wkPH/AfPyYmhi1btmg+RwWNBp8JxBjDuXPncLvd1NbWMjg4SEJCAs888wxp\naWlERkb69/V6vZw6dQq73U5NTRclJVspKfl1+vqsQLJ8eSt79lQwf/7xO0auFi9ezNatWzWfo4JO\ng88E0NHR4X8QsK2tjWnTppGZmYnNZiMuLu6Offv7+ykvL8fhcHDqVDguVx7l5Wv8I1c22zW2by8m\nJsbtH7kSEVJTU9myZYvmc9SEocEnSAYHBzlz5gxut5uzZ89ijGHZsmU88cQTrF69mrCwO3813d3d\nuFwuXK4SamrmU1z8DLW1ywAICfGSn99MVtYHREef8bcJCwsjKyuL7OxszeeoCUeDzzi7ceOG/0HA\nW7duMXPmTLZu3YrNZmPOnDkf2f/mzZs4HA7KysqpqlqJ0/kCTU3xAERE9FNQcJHU1INERV3xt5k+\nfTp5eXmaz1ETmgafcdDf38+pU6dwu900NjYiIqxatYr09HRWrlx5z9zLpUuXsNvtVFTUUV6+Aafz\ni7S0WMFp1qxe9u49z/Ll+wkLa/e3iYuLY8eOHZrPUZOCBp8AunLlCm63m8rKSnp6epgzZw5PPPEE\nGzZsYObMmR/Z3xhDfX29L4l8mbKyLJzOL9PVZRXXi4/v5plnThMXt5+wsD5/u+TkZPLz8zWfoyYV\nDT5jrLe3l8rKSjweD5cuXSI0NJTU1FTS09NJSkryD5EPNzg4SHV1NXa7ndOnb1NSkovL9bx/5Co5\nuZOCgpPMnn2Y0FBr5CokJIT09HS2bdum+Rw1KWnwGQPGGC5cuIDH46G6upr+/n4WLFhAQUEB69ev\nZ9q0afds19fXh9vtxuFwcOZMJMXFQyNXVnG9TZva2LbNwcyZLv/IVWRkJNu2bWPTpk2az1GTmgaf\nR3Dr1i0qKirweDy0tLQQERHBunXrSE9PZ9GiRfe8ygGr2F5xcTElJaXU1S2guHgfp05Zc7KJeNm1\nqwWb7TDR0bX+NrNnz2bnzp2sWbNG8zlqStDg84C8Xi8NDQ14PB5qa2v9RfP27dtHWlrax16NtLS0\n4HA4cLvLOXVqFQ7HCzQ1WXmayMhB9u69TnLyW0RFXfa3SUhI4MknnyQhISHg56bUeNLgM0rt7e3+\nIfL29namTZtGVlaWf97jj3PhwgXsdjuVlWeorNyA3f6HtLTMBWDWrH6ee+4iixf/GxER1siViLB6\n9Wr27NlDTExMwM9NqWDQ4PMxBgcHqaurw+PxcPbsWQBWrFjBrl27SElJ+ciDgMMZYzh9+jR2u526\nuqu43Vk4HE/T1WVNS71wYS/PPHOGBQveJiysF7AeCszMzGTHjh2az1FTngafe2hpacHtdlNRUUF3\ndzezZs0iLy8Pm83G7NmzP7btwMAAlZWV2O12Ghp6cbm2UFLym/T2Wu9lpaTcZteucubMOeQfuZo2\nbRo7duwgIyND8znqsRHQ4DOK0jkvAv/AhzMcfsMY84pv2+eAv/St/ztjzPcD2df+/n6qq6vxeDw0\nNTUREhJCSkoKNpuNFStW3Dco9PT0UFpaitPp5Ny5aTid2ygvT/OPXOXk3CInp5CYmA9HrmbPns3e\nvXtZseKeBUA+lJ9vfT927NFOUqkJJGDBx1c655sMK50jIm/dY1KwnxpjvnRX21isShaZgAHKfG3H\nvGTz8KJ5vb29xMbGsnPnTjZs2MCMGfev3NPR0eGbuKuU+vp47PZnqa1NBiAkxFBQ0EFq6jvMmnXa\n3yYhIYHnnnuOefPmjfXpKDVpBPLK50FL5wy3BzhkjGn1tT0EFAA/HouO9fT0UFlZidvt5sqVK4SF\nhfkfBExMTBxxiHxIWVkZR48eZc6cOVy40Exd3Srs9hdoarKmrI6M9PLMMzdYtuwNpk+3Rq5EhJSU\nFJ555pk7JgD7WENXPB984F/u7Ozklc9+lqSkJBobG8nLywOgsLDwjnUZGRl39LewsPAj65UKpkAG\nn9GWzvmUiORhFRj8M2PMhRHa3nOsebSlc4wxNDU14Xa7qampYWBggPj4eJ566inWrVtHVFTUfU/I\nGENjYyMHDhxgYGCAmzf7ePkNcAjFAAAKe0lEQVTlP6SlxbqCiYkZZN++CyQkvEFUVAdgVfTctGkT\nO3fuJDQ09L4/437a2tvp6OigqqoKYwyFhYUAH1k3PMgUFhbS0dHxkfVKBVOwE86/An7smyj+94Hv\nA594kAPcr3ROV1cXFRUVuN1uWltbiYyMZOPGjaSnp7Nw4cJR/Qyv10ttbS1FRUWcO9fKxYtJLFhw\nlWPH8mlpmUdCwgAFBaeIi3ubiAjrnauoqCg+8YlPkJmZed8rqREN5XiG5XyulZUx666rHPjolc9w\neXl5/isfpSaKoJbOMcbcGLb4Ch9WqGgG8u9qe2y0P9jr9VJfX4/b7eb06dN4vV4SExPJy8sjNTWV\n8PDwUR1naOIuu91OU9MALtdmSkrS6e21rpLWrOnnpZeOExd3jNBQL2BNT7p3715Wrlw52u4+kIyM\njHtevXzcFc1IbZQKpqCWzhGRhcaYocd59wFDFVDfA/6nr4QOwG7gL+73AwcHBzl69Cjl5eV0dHQQ\nHR1NTk4ONpvtgZK73d3dlJSUUFxcTFNTNE5nHuXl6xgctP64cnJ6SE9/n/nzy/wjV3FxcXzyk59k\nwYIFo/45o6ajXGoKCnbpnD8RkX1YJZFbgRd9bVtF5G+xAhjAV4aSzx/n6tWrFBYWsnLlSgoKCli1\natUD5Vna2tp8rz+4OXcunhMnnqW2NgUAEcOuXR2sWfMrYmPP+tusXLmSffv23XOKDKXUyKZU6ZyU\nlBTjcrke+JWEy5cv+6o/VHHmTDInTuTS1GQV14uMNBQUXGXFijeZNesqYE1nYbPZ2L17tz6JrB5r\nWjrHZ+bMmaMOPMYYGhoaOHHiBGfONFJVtZ4TJ77I9evWbVNMjJfdu+tZtuxXREd3AhAeHk5eXh6b\nN28ek5ErpR5nUyr4jMbg4CA1NTUUFRXR1NSG252Bw/EcnZ3WhFyLFg2yY0c5iYkHiYy0Rq6io6Mp\nKChg7dq1Dz9ypZS6w2MTfIYm7rLb7Vy65KW4OIeSkkz/yNWqVf1s2XKcxYtPEBZmjVzFxsayb98+\nkpKSgtl1paakKR98urq6cLlcFBcXc+nSDOz27VRUbPCPXGVm3mLjxkMkJFT4R64WL17Mvn377jtV\nhlLq4U3Z4HPjxg3sdjvl5eU0NS30jVytwRhBxJCX10Ja2rvExZ3zt0lNTWXPnj06J7JS42DKBZ+L\nFy9SVFREbW0dZ86s5MSJF2hsXApARIRhx45GUlLeITb2OmCNXG3atInt27ePONeyUmrsTang09LS\nwssvf4/KyrXY7X/AtWtWqeFZs7xs336KVasOMHNmF2CNXOXn57Np06ZRP/GslBo7U+o5n5iYVQZK\n6Oiwhtvj4gbIzS1h1apjREVZI1czZsxg165drF27VifuUuoR6XM+Ph0ds4AYli27TVbWByQnl/hH\nrubOnUtBQQErVqzQ4XKlJoApFXyiogb59Kd/xvLlpxi6qElMTGT37t1a/UGpCWZKBZ85c66ycqX1\nburq1avZtWsXsbGxQe6VUupeplTwEREyMzPZvn37qKZAVUoFz5QKPvHx8Tz99NPB7oZSahSm1HCP\nJpKVmjwCGnxEpEBE6kTkrIj853ts//ciUiMiJ0XksIgkDds2KCLlvq+3AtlPpdT4C3bpHA+QaYzp\nFpEvYk2j+hu+bbeNMRsD1T+lVHAF8srHXzrHGNMHDJXO8TPGHDXGdPsWnVhzNSulHgOBDD6jLn/j\n83lg/7DlKBEpFRGniDw3UiMRecm3X+n169cfrcdKqXEzIUa7ROSzWNVJtw9bnWSMaRaR5cAREak0\nxtTf3fZ+pXOUUhNTIK987ls6B0BEdgL/FdhnjOkdWm+MafZ9b8Aqm2MLYF+VUuMskMHHXzpHRCKw\nSufcMWolIjbg21iB59qw9XNEJNL3eR6Qy+jKLCulJolgl875B2AG8HPfMzpNxph9wBrg2yLixQqQ\nX71rlEwpNclNqSk1MjMzTWlpabC7odRj41Gm1JhSTzgrpSYPDT5KqaDQ4KOUCgoNPkqpoNDgo5QK\nCg0+Sqmg0OCjlAoKDT5KqaDQ4KOUCgoNPkqpoNDgo5QKCg0+Sqmg0OCjlAoKDT5KqaAIdumcSBH5\nqW97sYgsHbbtL3zr60RkTyD7qZQafwELPsNK5zwJpAKfEZHUu3b7PHDTGLMS+Cfgf/napmLNfJgG\nFAD/7DueUmqKCGrpHN/y932ffwE8IdaUhs8CPzHG9BpjzgFnfcdTSk0Rgaxeca/SOdkj7eObdrUd\nmOtb77yr7T3L7ojIS8BLvsVeEal69K5PSPOAlmB3IkCm8rnB1D6/lIdtOCFK5zyK4aVzRKT0Yad0\nnOj03CavqXx+IvLQ8xYHu3SOfx8RCQNigBujbKuUmsSCWjrHt/w53+dPA0eMNaP9W8DzvtGwZUAy\n4ApgX5VS4yzYpXO+C/xARM4CrVgBCt9+P8Oq1TUA/JExZnAUP/blQJzLBKHnNnlN5fN76HObUqVz\nlFKThz7hrJQKCg0+SqmgmHTB51Fe2ZgMRnF+L4rIdREp9339XjD6+aBE5FURuTbSc1hi+brvvE+K\nSPp49/FRjOL88kWkfdjv7a/Gu48PS0SWiMhREakRkWoR+dN77PPgvz9jzKT5wkpc1wPLgQigAki9\na58/BL7l+/w88NNg93uMz+9F4BvB7utDnFsekA5UjbD9KWA/IEAOUBzsPo/x+eUDbwe7nw95bguB\ndN/nmcDpe/y9fODf32S78nmUVzYmg9Gc36RkjCnEGtEcybPAvxqLE5gtIgvHp3ePbhTnN2kZYy4b\nY9y+z53AKT76xsED//4mW/C51ysbd/8h3PHKBjD0ysZkMJrzA/iU79L2FyKy5B7bJ6PRnvtktllE\nKkRkv4ikBbszD8OXxrABxXdteuDf32QLPgp+BSw1xqwHDvHhVZ6a2NxAkjFmA/B/gTeD3J8HJiIz\ngNeBLxtjOh71eJMt+DzKKxuTwX3PzxhzwxjT61t8BcgYp74F2pR+pcYY02GM6fJ9fhcIF5F5Qe7W\nqIlIOFbg+X/GmF/eY5cH/v1NtuDzKK9sTAb3Pb+77qP3Yd1/TwVvAb/tGzXJAdqNMZeD3amxIiLx\nQ7lHEcnC+rc3Kf5T9PX7u8ApY8zXRtjtgX9/k+qtdvMIr2xMBqM8vz8RkX1Yr520Yo1+TXgi8mOs\nEZ95InIR+GsgHMAY8y3gXawRk7NAN/A7wenpwxnF+X0a+KKIDAC3gecn0X+KucALQKWIlPvW/Rcg\nER7+96evVyilgmKy3XYppaYIDT5KqaDQ4KOUCgoNPkqpoNDgo5QKCg0+Kih8b0qfE5FY3/Ic3/LS\n4PZMjRcNPioojDEXgH8Bvupb9VXgZWPM+aB1So0rfc5HBY3vkf0y4FXgC8BGY0x/cHulxsukesJZ\nTS3GmH4R+Y/AAWC3Bp7Hi952qWB7ErgMrA12R9T40uCjgkZENgK7sGa++7PJNHmYenQafFRQ+N6U\n/hesuWGagH8A/ndwe6XGkwYfFSxfAJqMMYd8y/8MrBGR7UHskxpHOtqllAoKvfJRSgWFBh+lVFBo\n8FFKBYUGH6VUUGjwUUoFhQYfpVRQaPBRSgXF/we7m1W4Pphn+wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "k0AhsAmuJzT9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "Following is data describing characteristics of blog posts, with a target feature of how many comments will be posted in the following 24 hours.\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/BlogFeedback\n",
        "\n",
        "Investigate - you can try both linear and ridge. You can also sample to smaller data size and see if that makes ridge more important. Don't forget to scale!\n",
        "\n",
        "Focus on the training data, but if you want to load and compare to any of the test data files you can also do that.\n",
        "\n",
        "Note - Ridge may not be that fundamentally superior in this case. That's OK! It's still good to practice both, and see if you can find parameters or sample sizes where ridge does generalize and perform better.\n",
        "\n",
        "When you've fit models to your satisfaction, answer the following question:\n",
        "\n",
        "```\n",
        "Did you find cases where Ridge performed better? If so, describe (alpha parameter, sample size, any other relevant info/processing). If not, what do you think that tells you about the data?\n",
        "```\n",
        "\n",
        "You can create whatever plots, tables, or other results support your argument. In this case, your target audience is a fellow data scientist, *not* a layperson, so feel free to dig in!"
      ]
    },
    {
      "metadata": {
        "id": "TGsxtMAoTxk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "666e83e9-5717-4860-e1c3-151b51938675"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U seaborn"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: seaborn in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (0.22.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas>=0.15.2->seaborn) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (40.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HKKnNsttRpwI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xyvbvYN9DDmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "2f7564c0-8a05-4852-bf1c-235382a1d502"
      },
      "cell_type": "code",
      "source": [
        "!wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip'\n",
        "!unzip 'BlogFeedback.zip'"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-25 03:48:06--  https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2583605 (2.5M) [application/zip]\n",
            "Saving to: â€˜BlogFeedback.zip.4â€™\n",
            "\n",
            "BlogFeedback.zip.4  100%[===================>]   2.46M  1.51MB/s    in 1.6s    \n",
            "\n",
            "2019-01-25 03:48:08 (1.51 MB/s) - â€˜BlogFeedback.zip.4â€™ saved [2583605/2583605]\n",
            "\n",
            "Archive:  BlogFeedback.zip\n",
            "replace blogData_test-2012.02.01.00_00.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nsNVJDOJDP-C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "af86ac4e-0670-4492-d3ce-f803c3ea824b"
      },
      "cell_type": "code",
      "source": [
        "blog_df = pd.read_csv('blogData_train.csv', header=None)\n",
        "print(blog_df.shape)\n",
        "blog_df.head(5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(52397, 281)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 281 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0          1    2      3     4         5         6    7      8    9    \\\n",
              "0  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "1  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "2  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "3  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "4  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "\n",
              "   ...   271  272  273  274  275  276  277  278  279   280  \n",
              "0  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0  \n",
              "1  ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
              "2  ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
              "3  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0  \n",
              "4  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  27.0  \n",
              "\n",
              "[5 rows x 281 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "WWqd5WO0Emvr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41f87425-ebc1-4a42-bef1-e59784b21699"
      },
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)  # Unlimited columns\n",
        "print(blog_df.isnull().sum().sum())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "II6bKc87FLu0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "8b309f6b-dc32-427d-f4ae-e3b4d5440c1f"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scalar = StandardScaler()\n",
        "\n",
        "blog_df.iloc[:,0:279] = pd.DataFrame(scalar.fit_transform(blog_df.iloc[:,0:279]))\n",
        "blog_df.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>202</th>\n",
              "      <th>203</th>\n",
              "      <th>204</th>\n",
              "      <th>205</th>\n",
              "      <th>206</th>\n",
              "      <th>207</th>\n",
              "      <th>208</th>\n",
              "      <th>209</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>-0.00031</td>\n",
              "      <td>0.113595</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.374998</td>\n",
              "      <td>-0.11243</td>\n",
              "      <td>-0.004944</td>\n",
              "      <td>0.129936</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.245038</td>\n",
              "      <td>-0.161788</td>\n",
              "      <td>0.080132</td>\n",
              "      <td>0.119797</td>\n",
              "      <td>-0.480089</td>\n",
              "      <td>0.390503</td>\n",
              "      <td>0.140192</td>\n",
              "      <td>0.598229</td>\n",
              "      <td>1.06407</td>\n",
              "      <td>-0.029263</td>\n",
              "      <td>0.781584</td>\n",
              "      <td>-0.324229</td>\n",
              "      <td>0.581057</td>\n",
              "      <td>0.931022</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8758</td>\n",
              "      <td>-0.140659</td>\n",
              "      <td>0.578108</td>\n",
              "      <td>0.92603</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.876995</td>\n",
              "      <td>-0.022591</td>\n",
              "      <td>0.573526</td>\n",
              "      <td>1.073282</td>\n",
              "      <td>-0.029263</td>\n",
              "      <td>0.763658</td>\n",
              "      <td>-0.249724</td>\n",
              "      <td>0.499465</td>\n",
              "      <td>0.885857</td>\n",
              "      <td>-0.898355</td>\n",
              "      <td>0.910838</td>\n",
              "      <td>-0.015447</td>\n",
              "      <td>-0.337079</td>\n",
              "      <td>-0.229670</td>\n",
              "      <td>-0.249138</td>\n",
              "      <td>-0.346756</td>\n",
              "      <td>0.012103</td>\n",
              "      <td>-0.320777</td>\n",
              "      <td>-0.21839</td>\n",
              "      <td>-0.206545</td>\n",
              "      <td>-0.310545</td>\n",
              "      <td>-0.010048</td>\n",
              "      <td>-1.199772</td>\n",
              "      <td>-0.747296</td>\n",
              "      <td>-0.040546</td>\n",
              "      <td>-0.461678</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.739905</td>\n",
              "      <td>-0.432851</td>\n",
              "      <td>-0.564119</td>\n",
              "      <td>-0.049292</td>\n",
              "      <td>-0.058549</td>\n",
              "      <td>-0.190451</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.025482</td>\n",
              "      <td>-0.069517</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.128719</td>\n",
              "      <td>-0.037607</td>\n",
              "      <td>-0.536624</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.057888</td>\n",
              "      <td>-0.039591</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.14596</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.108441</td>\n",
              "      <td>-0.259927</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.06924</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.313762</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.019046</td>\n",
              "      <td>-0.033289</td>\n",
              "      <td>-1.226344</td>\n",
              "      <td>-0.287567</td>\n",
              "      <td>-0.176444</td>\n",
              "      <td>-0.229929</td>\n",
              "      <td>-0.055344</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.05447</td>\n",
              "      <td>-0.128028</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.160884</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.419249</td>\n",
              "      <td>-0.268361</td>\n",
              "      <td>-0.145755</td>\n",
              "      <td>-0.040782</td>\n",
              "      <td>-0.111464</td>\n",
              "      <td>-0.231901</td>\n",
              "      <td>-0.440503</td>\n",
              "      <td>-0.198537</td>\n",
              "      <td>-0.554032</td>\n",
              "      <td>-0.020956</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.102996</td>\n",
              "      <td>-0.021848</td>\n",
              "      <td>-0.079002</td>\n",
              "      <td>-0.123168</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.070618</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.024331</td>\n",
              "      <td>-0.205193</td>\n",
              "      <td>-0.167503</td>\n",
              "      <td>-0.222754</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>-0.17477</td>\n",
              "      <td>-0.581809</td>\n",
              "      <td>-0.288615</td>\n",
              "      <td>-0.284011</td>\n",
              "      <td>-0.049679</td>\n",
              "      <td>-1.36741</td>\n",
              "      <td>-0.088478</td>\n",
              "      <td>-0.060961</td>\n",
              "      <td>-0.055517</td>\n",
              "      <td>-0.071432</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.047711</td>\n",
              "      <td>-0.605361</td>\n",
              "      <td>-0.045658</td>\n",
              "      <td>-0.22762</td>\n",
              "      <td>-0.182952</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.092655</td>\n",
              "      <td>-0.708757</td>\n",
              "      <td>-0.354479</td>\n",
              "      <td>-0.034971</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.173081</td>\n",
              "      <td>-0.1669</td>\n",
              "      <td>-0.075118</td>\n",
              "      <td>-0.036313</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.016348</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.441769</td>\n",
              "      <td>-0.02899</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.131525</td>\n",
              "      <td>-0.054821</td>\n",
              "      <td>-0.034971</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.322745</td>\n",
              "      <td>-0.210391</td>\n",
              "      <td>-0.099729</td>\n",
              "      <td>-0.28846</td>\n",
              "      <td>-0.341206</td>\n",
              "      <td>-0.019046</td>\n",
              "      <td>-0.126791</td>\n",
              "      <td>-0.149668</td>\n",
              "      <td>-0.075885</td>\n",
              "      <td>-0.122367</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.349066</td>\n",
              "      <td>-0.128873</td>\n",
              "      <td>-0.386666</td>\n",
              "      <td>-0.421449</td>\n",
              "      <td>-0.14459</td>\n",
              "      <td>-0.13115</td>\n",
              "      <td>-0.289468</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.061589</td>\n",
              "      <td>-0.452126</td>\n",
              "      <td>-0.02899</td>\n",
              "      <td>-0.042394</td>\n",
              "      <td>-0.102712</td>\n",
              "      <td>-0.064488</td>\n",
              "      <td>-0.232631</td>\n",
              "      <td>-0.110406</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.600006</td>\n",
              "      <td>-0.051013</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.936565</td>\n",
              "      <td>-0.022281</td>\n",
              "      <td>-0.064488</td>\n",
              "      <td>-0.261138</td>\n",
              "      <td>-0.024331</td>\n",
              "      <td>-0.047104</td>\n",
              "      <td>-0.168224</td>\n",
              "      <td>-0.047711</td>\n",
              "      <td>-0.210044</td>\n",
              "      <td>-0.037861</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.080936</td>\n",
              "      <td>-0.747271</td>\n",
              "      <td>-0.079733</td>\n",
              "      <td>-0.396779</td>\n",
              "      <td>-0.039591</td>\n",
              "      <td>-0.364666</td>\n",
              "      <td>-0.065377</td>\n",
              "      <td>-0.801457</td>\n",
              "      <td>-0.431047</td>\n",
              "      <td>-0.209052</td>\n",
              "      <td>-0.037352</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.07356</td>\n",
              "      <td>-0.049292</td>\n",
              "      <td>-0.030595</td>\n",
              "      <td>-0.042619</td>\n",
              "      <td>-0.167443</td>\n",
              "      <td>-0.121156</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.144728</td>\n",
              "      <td>-1.328428</td>\n",
              "      <td>-0.375136</td>\n",
              "      <td>-0.526081</td>\n",
              "      <td>-0.088149</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.123884</td>\n",
              "      <td>-0.080216</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.16324</td>\n",
              "      <td>-0.122045</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.209499</td>\n",
              "      <td>-0.047911</td>\n",
              "      <td>-0.058712</td>\n",
              "      <td>-0.051201</td>\n",
              "      <td>-0.092341</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.358559</td>\n",
              "      <td>-0.361906</td>\n",
              "      <td>-0.400283</td>\n",
              "      <td>-0.445005</td>\n",
              "      <td>2.227325</td>\n",
              "      <td>-0.439113</td>\n",
              "      <td>-0.399224</td>\n",
              "      <td>-0.43518</td>\n",
              "      <td>-0.448846</td>\n",
              "      <td>-0.454696</td>\n",
              "      <td>2.272362</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>-0.00031</td>\n",
              "      <td>0.113595</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.374998</td>\n",
              "      <td>-0.11243</td>\n",
              "      <td>-0.004944</td>\n",
              "      <td>0.129936</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.245038</td>\n",
              "      <td>-0.161788</td>\n",
              "      <td>0.080132</td>\n",
              "      <td>0.119797</td>\n",
              "      <td>-0.480089</td>\n",
              "      <td>0.390503</td>\n",
              "      <td>0.140192</td>\n",
              "      <td>0.598229</td>\n",
              "      <td>1.06407</td>\n",
              "      <td>-0.029263</td>\n",
              "      <td>0.781584</td>\n",
              "      <td>-0.324229</td>\n",
              "      <td>0.581057</td>\n",
              "      <td>0.931022</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8758</td>\n",
              "      <td>-0.140659</td>\n",
              "      <td>0.578108</td>\n",
              "      <td>0.92603</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.876995</td>\n",
              "      <td>-0.022591</td>\n",
              "      <td>0.573526</td>\n",
              "      <td>1.073282</td>\n",
              "      <td>-0.029263</td>\n",
              "      <td>0.763658</td>\n",
              "      <td>-0.249724</td>\n",
              "      <td>0.499465</td>\n",
              "      <td>0.885857</td>\n",
              "      <td>-0.898355</td>\n",
              "      <td>0.910838</td>\n",
              "      <td>-0.015447</td>\n",
              "      <td>-0.301070</td>\n",
              "      <td>-0.229670</td>\n",
              "      <td>-0.178225</td>\n",
              "      <td>-0.315136</td>\n",
              "      <td>-0.045635</td>\n",
              "      <td>-0.320777</td>\n",
              "      <td>-0.21839</td>\n",
              "      <td>-0.206545</td>\n",
              "      <td>-0.310545</td>\n",
              "      <td>-0.010048</td>\n",
              "      <td>0.010755</td>\n",
              "      <td>-0.747296</td>\n",
              "      <td>-0.040546</td>\n",
              "      <td>-0.461678</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.739905</td>\n",
              "      <td>-0.432851</td>\n",
              "      <td>-0.564119</td>\n",
              "      <td>-0.049292</td>\n",
              "      <td>-0.058549</td>\n",
              "      <td>-0.190451</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.025482</td>\n",
              "      <td>-0.069517</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.128719</td>\n",
              "      <td>-0.037607</td>\n",
              "      <td>-0.536624</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.057888</td>\n",
              "      <td>-0.039591</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.14596</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.108441</td>\n",
              "      <td>-0.259927</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.06924</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.313762</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.019046</td>\n",
              "      <td>-0.033289</td>\n",
              "      <td>-1.226344</td>\n",
              "      <td>-0.287567</td>\n",
              "      <td>-0.176444</td>\n",
              "      <td>-0.229929</td>\n",
              "      <td>-0.055344</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.05447</td>\n",
              "      <td>-0.128028</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.160884</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.419249</td>\n",
              "      <td>-0.268361</td>\n",
              "      <td>-0.145755</td>\n",
              "      <td>-0.040782</td>\n",
              "      <td>-0.111464</td>\n",
              "      <td>-0.231901</td>\n",
              "      <td>-0.440503</td>\n",
              "      <td>-0.198537</td>\n",
              "      <td>-0.554032</td>\n",
              "      <td>-0.020956</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.102996</td>\n",
              "      <td>-0.021848</td>\n",
              "      <td>-0.079002</td>\n",
              "      <td>-0.123168</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.070618</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.024331</td>\n",
              "      <td>-0.205193</td>\n",
              "      <td>-0.167503</td>\n",
              "      <td>-0.222754</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>-0.17477</td>\n",
              "      <td>-0.581809</td>\n",
              "      <td>-0.288615</td>\n",
              "      <td>-0.284011</td>\n",
              "      <td>-0.049679</td>\n",
              "      <td>-1.36741</td>\n",
              "      <td>-0.088478</td>\n",
              "      <td>-0.060961</td>\n",
              "      <td>-0.055517</td>\n",
              "      <td>-0.071432</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.047711</td>\n",
              "      <td>-0.605361</td>\n",
              "      <td>-0.045658</td>\n",
              "      <td>-0.22762</td>\n",
              "      <td>-0.182952</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.092655</td>\n",
              "      <td>-0.708757</td>\n",
              "      <td>-0.354479</td>\n",
              "      <td>-0.034971</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.173081</td>\n",
              "      <td>-0.1669</td>\n",
              "      <td>-0.075118</td>\n",
              "      <td>-0.036313</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.016348</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.441769</td>\n",
              "      <td>-0.02899</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.131525</td>\n",
              "      <td>-0.054821</td>\n",
              "      <td>-0.034971</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.322745</td>\n",
              "      <td>-0.210391</td>\n",
              "      <td>-0.099729</td>\n",
              "      <td>-0.28846</td>\n",
              "      <td>-0.341206</td>\n",
              "      <td>-0.019046</td>\n",
              "      <td>-0.126791</td>\n",
              "      <td>-0.149668</td>\n",
              "      <td>-0.075885</td>\n",
              "      <td>-0.122367</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.349066</td>\n",
              "      <td>-0.128873</td>\n",
              "      <td>-0.386666</td>\n",
              "      <td>-0.421449</td>\n",
              "      <td>-0.14459</td>\n",
              "      <td>-0.13115</td>\n",
              "      <td>-0.289468</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.061589</td>\n",
              "      <td>-0.452126</td>\n",
              "      <td>-0.02899</td>\n",
              "      <td>-0.042394</td>\n",
              "      <td>-0.102712</td>\n",
              "      <td>-0.064488</td>\n",
              "      <td>-0.232631</td>\n",
              "      <td>-0.110406</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.600006</td>\n",
              "      <td>-0.051013</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.936565</td>\n",
              "      <td>-0.022281</td>\n",
              "      <td>-0.064488</td>\n",
              "      <td>-0.261138</td>\n",
              "      <td>-0.024331</td>\n",
              "      <td>-0.047104</td>\n",
              "      <td>-0.168224</td>\n",
              "      <td>-0.047711</td>\n",
              "      <td>-0.210044</td>\n",
              "      <td>-0.037861</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.080936</td>\n",
              "      <td>-0.747271</td>\n",
              "      <td>-0.079733</td>\n",
              "      <td>-0.396779</td>\n",
              "      <td>-0.039591</td>\n",
              "      <td>-0.364666</td>\n",
              "      <td>-0.065377</td>\n",
              "      <td>-0.801457</td>\n",
              "      <td>-0.431047</td>\n",
              "      <td>-0.209052</td>\n",
              "      <td>-0.037352</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.07356</td>\n",
              "      <td>-0.049292</td>\n",
              "      <td>-0.030595</td>\n",
              "      <td>-0.042619</td>\n",
              "      <td>-0.167443</td>\n",
              "      <td>-0.121156</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.144728</td>\n",
              "      <td>-1.328428</td>\n",
              "      <td>-0.375136</td>\n",
              "      <td>-0.526081</td>\n",
              "      <td>-0.088149</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.123884</td>\n",
              "      <td>-0.080216</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.16324</td>\n",
              "      <td>-0.122045</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.209499</td>\n",
              "      <td>-0.047911</td>\n",
              "      <td>-0.058712</td>\n",
              "      <td>-0.051201</td>\n",
              "      <td>-0.092341</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.358559</td>\n",
              "      <td>-0.361906</td>\n",
              "      <td>-0.400283</td>\n",
              "      <td>-0.445005</td>\n",
              "      <td>2.227325</td>\n",
              "      <td>-0.439113</td>\n",
              "      <td>-0.399224</td>\n",
              "      <td>-0.43518</td>\n",
              "      <td>-0.448846</td>\n",
              "      <td>2.199274</td>\n",
              "      <td>-0.440071</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>-0.00031</td>\n",
              "      <td>0.113595</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.374998</td>\n",
              "      <td>-0.11243</td>\n",
              "      <td>-0.004944</td>\n",
              "      <td>0.129936</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.245038</td>\n",
              "      <td>-0.161788</td>\n",
              "      <td>0.080132</td>\n",
              "      <td>0.119797</td>\n",
              "      <td>-0.480089</td>\n",
              "      <td>0.390503</td>\n",
              "      <td>0.140192</td>\n",
              "      <td>0.598229</td>\n",
              "      <td>1.06407</td>\n",
              "      <td>-0.029263</td>\n",
              "      <td>0.781584</td>\n",
              "      <td>-0.324229</td>\n",
              "      <td>0.581057</td>\n",
              "      <td>0.931022</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8758</td>\n",
              "      <td>-0.140659</td>\n",
              "      <td>0.578108</td>\n",
              "      <td>0.92603</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.876995</td>\n",
              "      <td>-0.022591</td>\n",
              "      <td>0.573526</td>\n",
              "      <td>1.073282</td>\n",
              "      <td>-0.029263</td>\n",
              "      <td>0.763658</td>\n",
              "      <td>-0.249724</td>\n",
              "      <td>0.499465</td>\n",
              "      <td>0.885857</td>\n",
              "      <td>-0.898355</td>\n",
              "      <td>0.910838</td>\n",
              "      <td>-0.015447</td>\n",
              "      <td>-0.301070</td>\n",
              "      <td>-0.229670</td>\n",
              "      <td>-0.178225</td>\n",
              "      <td>-0.315136</td>\n",
              "      <td>-0.045635</td>\n",
              "      <td>-0.320777</td>\n",
              "      <td>-0.21839</td>\n",
              "      <td>-0.206545</td>\n",
              "      <td>-0.310545</td>\n",
              "      <td>-0.010048</td>\n",
              "      <td>0.010755</td>\n",
              "      <td>-0.747296</td>\n",
              "      <td>-0.040546</td>\n",
              "      <td>-0.461678</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.739905</td>\n",
              "      <td>-0.432851</td>\n",
              "      <td>-0.564119</td>\n",
              "      <td>-0.049292</td>\n",
              "      <td>-0.058549</td>\n",
              "      <td>-0.190451</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.025482</td>\n",
              "      <td>-0.069517</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.128719</td>\n",
              "      <td>-0.037607</td>\n",
              "      <td>-0.536624</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.057888</td>\n",
              "      <td>-0.039591</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.14596</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.108441</td>\n",
              "      <td>-0.259927</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.06924</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.313762</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.019046</td>\n",
              "      <td>-0.033289</td>\n",
              "      <td>-1.226344</td>\n",
              "      <td>-0.287567</td>\n",
              "      <td>-0.176444</td>\n",
              "      <td>-0.229929</td>\n",
              "      <td>-0.055344</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.05447</td>\n",
              "      <td>-0.128028</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.160884</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.419249</td>\n",
              "      <td>-0.268361</td>\n",
              "      <td>-0.145755</td>\n",
              "      <td>-0.040782</td>\n",
              "      <td>-0.111464</td>\n",
              "      <td>-0.231901</td>\n",
              "      <td>-0.440503</td>\n",
              "      <td>-0.198537</td>\n",
              "      <td>-0.554032</td>\n",
              "      <td>-0.020956</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.102996</td>\n",
              "      <td>-0.021848</td>\n",
              "      <td>-0.079002</td>\n",
              "      <td>-0.123168</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.070618</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.024331</td>\n",
              "      <td>-0.205193</td>\n",
              "      <td>-0.167503</td>\n",
              "      <td>-0.222754</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>-0.17477</td>\n",
              "      <td>-0.581809</td>\n",
              "      <td>-0.288615</td>\n",
              "      <td>-0.284011</td>\n",
              "      <td>-0.049679</td>\n",
              "      <td>-1.36741</td>\n",
              "      <td>-0.088478</td>\n",
              "      <td>-0.060961</td>\n",
              "      <td>-0.055517</td>\n",
              "      <td>-0.071432</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.047711</td>\n",
              "      <td>-0.605361</td>\n",
              "      <td>-0.045658</td>\n",
              "      <td>-0.22762</td>\n",
              "      <td>-0.182952</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.092655</td>\n",
              "      <td>-0.708757</td>\n",
              "      <td>-0.354479</td>\n",
              "      <td>-0.034971</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.173081</td>\n",
              "      <td>-0.1669</td>\n",
              "      <td>-0.075118</td>\n",
              "      <td>-0.036313</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.016348</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.441769</td>\n",
              "      <td>-0.02899</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.131525</td>\n",
              "      <td>-0.054821</td>\n",
              "      <td>-0.034971</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.322745</td>\n",
              "      <td>-0.210391</td>\n",
              "      <td>-0.099729</td>\n",
              "      <td>-0.28846</td>\n",
              "      <td>-0.341206</td>\n",
              "      <td>-0.019046</td>\n",
              "      <td>-0.126791</td>\n",
              "      <td>-0.149668</td>\n",
              "      <td>-0.075885</td>\n",
              "      <td>-0.122367</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.349066</td>\n",
              "      <td>-0.128873</td>\n",
              "      <td>-0.386666</td>\n",
              "      <td>-0.421449</td>\n",
              "      <td>-0.14459</td>\n",
              "      <td>-0.13115</td>\n",
              "      <td>-0.289468</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.061589</td>\n",
              "      <td>-0.452126</td>\n",
              "      <td>-0.02899</td>\n",
              "      <td>-0.042394</td>\n",
              "      <td>-0.102712</td>\n",
              "      <td>-0.064488</td>\n",
              "      <td>-0.232631</td>\n",
              "      <td>-0.110406</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.600006</td>\n",
              "      <td>-0.051013</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.936565</td>\n",
              "      <td>-0.022281</td>\n",
              "      <td>-0.064488</td>\n",
              "      <td>-0.261138</td>\n",
              "      <td>-0.024331</td>\n",
              "      <td>-0.047104</td>\n",
              "      <td>-0.168224</td>\n",
              "      <td>-0.047711</td>\n",
              "      <td>-0.210044</td>\n",
              "      <td>-0.037861</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.080936</td>\n",
              "      <td>-0.747271</td>\n",
              "      <td>-0.079733</td>\n",
              "      <td>-0.396779</td>\n",
              "      <td>-0.039591</td>\n",
              "      <td>-0.364666</td>\n",
              "      <td>-0.065377</td>\n",
              "      <td>-0.801457</td>\n",
              "      <td>-0.431047</td>\n",
              "      <td>-0.209052</td>\n",
              "      <td>-0.037352</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.07356</td>\n",
              "      <td>-0.049292</td>\n",
              "      <td>-0.030595</td>\n",
              "      <td>-0.042619</td>\n",
              "      <td>-0.167443</td>\n",
              "      <td>-0.121156</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.144728</td>\n",
              "      <td>-1.328428</td>\n",
              "      <td>-0.375136</td>\n",
              "      <td>-0.526081</td>\n",
              "      <td>-0.088149</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.123884</td>\n",
              "      <td>-0.080216</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.16324</td>\n",
              "      <td>-0.122045</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.209499</td>\n",
              "      <td>-0.047911</td>\n",
              "      <td>-0.058712</td>\n",
              "      <td>-0.051201</td>\n",
              "      <td>-0.092341</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.358559</td>\n",
              "      <td>-0.361906</td>\n",
              "      <td>-0.400283</td>\n",
              "      <td>-0.445005</td>\n",
              "      <td>2.227325</td>\n",
              "      <td>-0.439113</td>\n",
              "      <td>-0.399224</td>\n",
              "      <td>-0.43518</td>\n",
              "      <td>-0.448846</td>\n",
              "      <td>2.199274</td>\n",
              "      <td>-0.440071</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>-0.00031</td>\n",
              "      <td>0.113595</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.374998</td>\n",
              "      <td>-0.11243</td>\n",
              "      <td>-0.004944</td>\n",
              "      <td>0.129936</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.245038</td>\n",
              "      <td>-0.161788</td>\n",
              "      <td>0.080132</td>\n",
              "      <td>0.119797</td>\n",
              "      <td>-0.480089</td>\n",
              "      <td>0.390503</td>\n",
              "      <td>0.140192</td>\n",
              "      <td>0.598229</td>\n",
              "      <td>1.06407</td>\n",
              "      <td>-0.029263</td>\n",
              "      <td>0.781584</td>\n",
              "      <td>-0.324229</td>\n",
              "      <td>0.581057</td>\n",
              "      <td>0.931022</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8758</td>\n",
              "      <td>-0.140659</td>\n",
              "      <td>0.578108</td>\n",
              "      <td>0.92603</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.876995</td>\n",
              "      <td>-0.022591</td>\n",
              "      <td>0.573526</td>\n",
              "      <td>1.073282</td>\n",
              "      <td>-0.029263</td>\n",
              "      <td>0.763658</td>\n",
              "      <td>-0.249724</td>\n",
              "      <td>0.499465</td>\n",
              "      <td>0.885857</td>\n",
              "      <td>-0.898355</td>\n",
              "      <td>0.910838</td>\n",
              "      <td>-0.015447</td>\n",
              "      <td>-0.337079</td>\n",
              "      <td>-0.229670</td>\n",
              "      <td>-0.249138</td>\n",
              "      <td>-0.346756</td>\n",
              "      <td>0.012103</td>\n",
              "      <td>-0.320777</td>\n",
              "      <td>-0.21839</td>\n",
              "      <td>-0.206545</td>\n",
              "      <td>-0.310545</td>\n",
              "      <td>-0.010048</td>\n",
              "      <td>-1.199772</td>\n",
              "      <td>-0.747296</td>\n",
              "      <td>-0.040546</td>\n",
              "      <td>-0.461678</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.739905</td>\n",
              "      <td>-0.432851</td>\n",
              "      <td>-0.564119</td>\n",
              "      <td>-0.049292</td>\n",
              "      <td>-0.058549</td>\n",
              "      <td>-0.190451</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.025482</td>\n",
              "      <td>-0.069517</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.128719</td>\n",
              "      <td>-0.037607</td>\n",
              "      <td>-0.536624</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.057888</td>\n",
              "      <td>-0.039591</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.14596</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.108441</td>\n",
              "      <td>-0.259927</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.06924</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.313762</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.019046</td>\n",
              "      <td>-0.033289</td>\n",
              "      <td>-1.226344</td>\n",
              "      <td>-0.287567</td>\n",
              "      <td>-0.176444</td>\n",
              "      <td>-0.229929</td>\n",
              "      <td>-0.055344</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.05447</td>\n",
              "      <td>-0.128028</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.160884</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.419249</td>\n",
              "      <td>-0.268361</td>\n",
              "      <td>-0.145755</td>\n",
              "      <td>-0.040782</td>\n",
              "      <td>-0.111464</td>\n",
              "      <td>-0.231901</td>\n",
              "      <td>-0.440503</td>\n",
              "      <td>-0.198537</td>\n",
              "      <td>-0.554032</td>\n",
              "      <td>-0.020956</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.102996</td>\n",
              "      <td>-0.021848</td>\n",
              "      <td>-0.079002</td>\n",
              "      <td>-0.123168</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.070618</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.024331</td>\n",
              "      <td>-0.205193</td>\n",
              "      <td>-0.167503</td>\n",
              "      <td>-0.222754</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>-0.17477</td>\n",
              "      <td>-0.581809</td>\n",
              "      <td>-0.288615</td>\n",
              "      <td>-0.284011</td>\n",
              "      <td>-0.049679</td>\n",
              "      <td>-1.36741</td>\n",
              "      <td>-0.088478</td>\n",
              "      <td>-0.060961</td>\n",
              "      <td>-0.055517</td>\n",
              "      <td>-0.071432</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.047711</td>\n",
              "      <td>-0.605361</td>\n",
              "      <td>-0.045658</td>\n",
              "      <td>-0.22762</td>\n",
              "      <td>-0.182952</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.092655</td>\n",
              "      <td>-0.708757</td>\n",
              "      <td>-0.354479</td>\n",
              "      <td>-0.034971</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.173081</td>\n",
              "      <td>-0.1669</td>\n",
              "      <td>-0.075118</td>\n",
              "      <td>-0.036313</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.016348</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.441769</td>\n",
              "      <td>-0.02899</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.131525</td>\n",
              "      <td>-0.054821</td>\n",
              "      <td>-0.034971</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.322745</td>\n",
              "      <td>-0.210391</td>\n",
              "      <td>-0.099729</td>\n",
              "      <td>-0.28846</td>\n",
              "      <td>-0.341206</td>\n",
              "      <td>-0.019046</td>\n",
              "      <td>-0.126791</td>\n",
              "      <td>-0.149668</td>\n",
              "      <td>-0.075885</td>\n",
              "      <td>-0.122367</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.349066</td>\n",
              "      <td>-0.128873</td>\n",
              "      <td>-0.386666</td>\n",
              "      <td>-0.421449</td>\n",
              "      <td>-0.14459</td>\n",
              "      <td>-0.13115</td>\n",
              "      <td>-0.289468</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.061589</td>\n",
              "      <td>-0.452126</td>\n",
              "      <td>-0.02899</td>\n",
              "      <td>-0.042394</td>\n",
              "      <td>-0.102712</td>\n",
              "      <td>-0.064488</td>\n",
              "      <td>-0.232631</td>\n",
              "      <td>-0.110406</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.600006</td>\n",
              "      <td>-0.051013</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.936565</td>\n",
              "      <td>-0.022281</td>\n",
              "      <td>-0.064488</td>\n",
              "      <td>-0.261138</td>\n",
              "      <td>-0.024331</td>\n",
              "      <td>-0.047104</td>\n",
              "      <td>-0.168224</td>\n",
              "      <td>-0.047711</td>\n",
              "      <td>-0.210044</td>\n",
              "      <td>-0.037861</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.080936</td>\n",
              "      <td>-0.747271</td>\n",
              "      <td>-0.079733</td>\n",
              "      <td>-0.396779</td>\n",
              "      <td>-0.039591</td>\n",
              "      <td>-0.364666</td>\n",
              "      <td>-0.065377</td>\n",
              "      <td>-0.801457</td>\n",
              "      <td>-0.431047</td>\n",
              "      <td>-0.209052</td>\n",
              "      <td>-0.037352</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.07356</td>\n",
              "      <td>-0.049292</td>\n",
              "      <td>-0.030595</td>\n",
              "      <td>-0.042619</td>\n",
              "      <td>-0.167443</td>\n",
              "      <td>-0.121156</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.144728</td>\n",
              "      <td>-1.328428</td>\n",
              "      <td>-0.375136</td>\n",
              "      <td>-0.526081</td>\n",
              "      <td>-0.088149</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.123884</td>\n",
              "      <td>-0.080216</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.16324</td>\n",
              "      <td>-0.122045</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.209499</td>\n",
              "      <td>-0.047911</td>\n",
              "      <td>-0.058712</td>\n",
              "      <td>-0.051201</td>\n",
              "      <td>-0.092341</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.358559</td>\n",
              "      <td>-0.361906</td>\n",
              "      <td>-0.400283</td>\n",
              "      <td>-0.445005</td>\n",
              "      <td>2.227325</td>\n",
              "      <td>-0.439113</td>\n",
              "      <td>-0.399224</td>\n",
              "      <td>-0.43518</td>\n",
              "      <td>-0.448846</td>\n",
              "      <td>-0.454696</td>\n",
              "      <td>2.272362</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>-0.00031</td>\n",
              "      <td>0.113595</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.374998</td>\n",
              "      <td>-0.11243</td>\n",
              "      <td>-0.004944</td>\n",
              "      <td>0.129936</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.245038</td>\n",
              "      <td>-0.161788</td>\n",
              "      <td>0.080132</td>\n",
              "      <td>0.119797</td>\n",
              "      <td>-0.480089</td>\n",
              "      <td>0.390503</td>\n",
              "      <td>0.140192</td>\n",
              "      <td>0.598229</td>\n",
              "      <td>1.06407</td>\n",
              "      <td>-0.029263</td>\n",
              "      <td>0.781584</td>\n",
              "      <td>-0.324229</td>\n",
              "      <td>0.581057</td>\n",
              "      <td>0.931022</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8758</td>\n",
              "      <td>-0.140659</td>\n",
              "      <td>0.578108</td>\n",
              "      <td>0.92603</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.876995</td>\n",
              "      <td>-0.022591</td>\n",
              "      <td>0.573526</td>\n",
              "      <td>1.073282</td>\n",
              "      <td>-0.029263</td>\n",
              "      <td>0.763658</td>\n",
              "      <td>-0.249724</td>\n",
              "      <td>0.499465</td>\n",
              "      <td>0.885857</td>\n",
              "      <td>-0.898355</td>\n",
              "      <td>0.910838</td>\n",
              "      <td>-0.015447</td>\n",
              "      <td>-0.328076</td>\n",
              "      <td>-0.247051</td>\n",
              "      <td>-0.213681</td>\n",
              "      <td>-0.346756</td>\n",
              "      <td>-0.031200</td>\n",
              "      <td>-0.320777</td>\n",
              "      <td>-0.21839</td>\n",
              "      <td>-0.206545</td>\n",
              "      <td>-0.310545</td>\n",
              "      <td>-0.010048</td>\n",
              "      <td>-0.037666</td>\n",
              "      <td>-0.747296</td>\n",
              "      <td>-0.040546</td>\n",
              "      <td>-0.461678</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.739905</td>\n",
              "      <td>-0.432851</td>\n",
              "      <td>-0.564119</td>\n",
              "      <td>-0.049292</td>\n",
              "      <td>-0.058549</td>\n",
              "      <td>-0.190451</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.025482</td>\n",
              "      <td>-0.069517</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.128719</td>\n",
              "      <td>-0.037607</td>\n",
              "      <td>-0.536624</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.057888</td>\n",
              "      <td>-0.039591</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.14596</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.108441</td>\n",
              "      <td>-0.259927</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.06924</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.313762</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.019046</td>\n",
              "      <td>-0.033289</td>\n",
              "      <td>-1.226344</td>\n",
              "      <td>-0.287567</td>\n",
              "      <td>-0.176444</td>\n",
              "      <td>-0.229929</td>\n",
              "      <td>-0.055344</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.05447</td>\n",
              "      <td>-0.128028</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.160884</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.419249</td>\n",
              "      <td>-0.268361</td>\n",
              "      <td>-0.145755</td>\n",
              "      <td>-0.040782</td>\n",
              "      <td>-0.111464</td>\n",
              "      <td>-0.231901</td>\n",
              "      <td>-0.440503</td>\n",
              "      <td>-0.198537</td>\n",
              "      <td>-0.554032</td>\n",
              "      <td>-0.020956</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.102996</td>\n",
              "      <td>-0.021848</td>\n",
              "      <td>-0.079002</td>\n",
              "      <td>-0.123168</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.070618</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.024331</td>\n",
              "      <td>-0.205193</td>\n",
              "      <td>-0.167503</td>\n",
              "      <td>-0.222754</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>-0.17477</td>\n",
              "      <td>-0.581809</td>\n",
              "      <td>-0.288615</td>\n",
              "      <td>-0.284011</td>\n",
              "      <td>-0.049679</td>\n",
              "      <td>-1.36741</td>\n",
              "      <td>-0.088478</td>\n",
              "      <td>-0.060961</td>\n",
              "      <td>-0.055517</td>\n",
              "      <td>-0.071432</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.047711</td>\n",
              "      <td>-0.605361</td>\n",
              "      <td>-0.045658</td>\n",
              "      <td>-0.22762</td>\n",
              "      <td>-0.182952</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.092655</td>\n",
              "      <td>-0.708757</td>\n",
              "      <td>-0.354479</td>\n",
              "      <td>-0.034971</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.173081</td>\n",
              "      <td>-0.1669</td>\n",
              "      <td>-0.075118</td>\n",
              "      <td>-0.036313</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.016348</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.441769</td>\n",
              "      <td>-0.02899</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.131525</td>\n",
              "      <td>-0.054821</td>\n",
              "      <td>-0.034971</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>-0.020495</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.322745</td>\n",
              "      <td>-0.210391</td>\n",
              "      <td>-0.099729</td>\n",
              "      <td>-0.28846</td>\n",
              "      <td>-0.341206</td>\n",
              "      <td>-0.019046</td>\n",
              "      <td>-0.126791</td>\n",
              "      <td>-0.149668</td>\n",
              "      <td>-0.075885</td>\n",
              "      <td>-0.122367</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.349066</td>\n",
              "      <td>-0.128873</td>\n",
              "      <td>-0.386666</td>\n",
              "      <td>-0.421449</td>\n",
              "      <td>-0.14459</td>\n",
              "      <td>-0.13115</td>\n",
              "      <td>-0.289468</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.061589</td>\n",
              "      <td>-0.452126</td>\n",
              "      <td>-0.02899</td>\n",
              "      <td>-0.042394</td>\n",
              "      <td>-0.102712</td>\n",
              "      <td>-0.064488</td>\n",
              "      <td>-0.232631</td>\n",
              "      <td>-0.110406</td>\n",
              "      <td>-0.035782</td>\n",
              "      <td>-0.600006</td>\n",
              "      <td>-0.051013</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.936565</td>\n",
              "      <td>-0.022281</td>\n",
              "      <td>-0.064488</td>\n",
              "      <td>-0.261138</td>\n",
              "      <td>-0.024331</td>\n",
              "      <td>-0.047104</td>\n",
              "      <td>-0.168224</td>\n",
              "      <td>-0.047711</td>\n",
              "      <td>-0.210044</td>\n",
              "      <td>-0.037861</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.080936</td>\n",
              "      <td>-0.747271</td>\n",
              "      <td>-0.079733</td>\n",
              "      <td>-0.396779</td>\n",
              "      <td>-0.039591</td>\n",
              "      <td>-0.364666</td>\n",
              "      <td>-0.065377</td>\n",
              "      <td>-0.801457</td>\n",
              "      <td>-0.431047</td>\n",
              "      <td>-0.209052</td>\n",
              "      <td>-0.037352</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.07356</td>\n",
              "      <td>-0.049292</td>\n",
              "      <td>-0.030595</td>\n",
              "      <td>-0.042619</td>\n",
              "      <td>-0.167443</td>\n",
              "      <td>-0.121156</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.011559</td>\n",
              "      <td>-0.144728</td>\n",
              "      <td>-1.328428</td>\n",
              "      <td>-0.375136</td>\n",
              "      <td>-0.526081</td>\n",
              "      <td>-0.088149</td>\n",
              "      <td>-0.008738</td>\n",
              "      <td>-0.123884</td>\n",
              "      <td>-0.080216</td>\n",
              "      <td>-0.013816</td>\n",
              "      <td>-0.16324</td>\n",
              "      <td>-0.122045</td>\n",
              "      <td>-0.015753</td>\n",
              "      <td>-0.209499</td>\n",
              "      <td>-0.047911</td>\n",
              "      <td>-0.058712</td>\n",
              "      <td>-0.051201</td>\n",
              "      <td>-0.092341</td>\n",
              "      <td>-0.017477</td>\n",
              "      <td>-0.358559</td>\n",
              "      <td>-0.361906</td>\n",
              "      <td>-0.400283</td>\n",
              "      <td>-0.445005</td>\n",
              "      <td>-0.448969</td>\n",
              "      <td>2.277320</td>\n",
              "      <td>-0.399224</td>\n",
              "      <td>-0.43518</td>\n",
              "      <td>-0.448846</td>\n",
              "      <td>-0.454696</td>\n",
              "      <td>2.272362</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "1  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "2  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "3  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "4  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "\n",
              "        7         8         9        10        11   12        13       14   \\\n",
              "0 -0.020836  0.368246 -0.119031 -0.00031  0.113595  0.0  0.374998 -0.11243   \n",
              "1 -0.020836  0.368246 -0.119031 -0.00031  0.113595  0.0  0.374998 -0.11243   \n",
              "2 -0.020836  0.368246 -0.119031 -0.00031  0.113595  0.0  0.374998 -0.11243   \n",
              "3 -0.020836  0.368246 -0.119031 -0.00031  0.113595  0.0  0.374998 -0.11243   \n",
              "4 -0.020836  0.368246 -0.119031 -0.00031  0.113595  0.0  0.374998 -0.11243   \n",
              "\n",
              "        15        16        17        18        19        20        21   \\\n",
              "0 -0.004944  0.129936 -0.052468  0.245038 -0.161788  0.080132  0.119797   \n",
              "1 -0.004944  0.129936 -0.052468  0.245038 -0.161788  0.080132  0.119797   \n",
              "2 -0.004944  0.129936 -0.052468  0.245038 -0.161788  0.080132  0.119797   \n",
              "3 -0.004944  0.129936 -0.052468  0.245038 -0.161788  0.080132  0.119797   \n",
              "4 -0.004944  0.129936 -0.052468  0.245038 -0.161788  0.080132  0.119797   \n",
              "\n",
              "        22        23        24        25       26        27        28   \\\n",
              "0 -0.480089  0.390503  0.140192  0.598229  1.06407 -0.029263  0.781584   \n",
              "1 -0.480089  0.390503  0.140192  0.598229  1.06407 -0.029263  0.781584   \n",
              "2 -0.480089  0.390503  0.140192  0.598229  1.06407 -0.029263  0.781584   \n",
              "3 -0.480089  0.390503  0.140192  0.598229  1.06407 -0.029263  0.781584   \n",
              "4 -0.480089  0.390503  0.140192  0.598229  1.06407 -0.029263  0.781584   \n",
              "\n",
              "        29        30        31   32      33        34        35       36   \\\n",
              "0 -0.324229  0.581057  0.931022  0.0  0.8758 -0.140659  0.578108  0.92603   \n",
              "1 -0.324229  0.581057  0.931022  0.0  0.8758 -0.140659  0.578108  0.92603   \n",
              "2 -0.324229  0.581057  0.931022  0.0  0.8758 -0.140659  0.578108  0.92603   \n",
              "3 -0.324229  0.581057  0.931022  0.0  0.8758 -0.140659  0.578108  0.92603   \n",
              "4 -0.324229  0.581057  0.931022  0.0  0.8758 -0.140659  0.578108  0.92603   \n",
              "\n",
              "   37        38        39        40        41        42        43        44   \\\n",
              "0  0.0  0.876995 -0.022591  0.573526  1.073282 -0.029263  0.763658 -0.249724   \n",
              "1  0.0  0.876995 -0.022591  0.573526  1.073282 -0.029263  0.763658 -0.249724   \n",
              "2  0.0  0.876995 -0.022591  0.573526  1.073282 -0.029263  0.763658 -0.249724   \n",
              "3  0.0  0.876995 -0.022591  0.573526  1.073282 -0.029263  0.763658 -0.249724   \n",
              "4  0.0  0.876995 -0.022591  0.573526  1.073282 -0.029263  0.763658 -0.249724   \n",
              "\n",
              "        45        46        47        48        49        50        51   \\\n",
              "0  0.499465  0.885857 -0.898355  0.910838 -0.015447 -0.337079 -0.229670   \n",
              "1  0.499465  0.885857 -0.898355  0.910838 -0.015447 -0.301070 -0.229670   \n",
              "2  0.499465  0.885857 -0.898355  0.910838 -0.015447 -0.301070 -0.229670   \n",
              "3  0.499465  0.885857 -0.898355  0.910838 -0.015447 -0.337079 -0.229670   \n",
              "4  0.499465  0.885857 -0.898355  0.910838 -0.015447 -0.328076 -0.247051   \n",
              "\n",
              "        52        53        54        55       56        57        58   \\\n",
              "0 -0.249138 -0.346756  0.012103 -0.320777 -0.21839 -0.206545 -0.310545   \n",
              "1 -0.178225 -0.315136 -0.045635 -0.320777 -0.21839 -0.206545 -0.310545   \n",
              "2 -0.178225 -0.315136 -0.045635 -0.320777 -0.21839 -0.206545 -0.310545   \n",
              "3 -0.249138 -0.346756  0.012103 -0.320777 -0.21839 -0.206545 -0.310545   \n",
              "4 -0.213681 -0.346756 -0.031200 -0.320777 -0.21839 -0.206545 -0.310545   \n",
              "\n",
              "        59        60        61        62        63        64        65   \\\n",
              "0 -0.010048 -1.199772 -0.747296 -0.040546 -0.461678 -0.008738 -0.017477   \n",
              "1 -0.010048  0.010755 -0.747296 -0.040546 -0.461678 -0.008738 -0.017477   \n",
              "2 -0.010048  0.010755 -0.747296 -0.040546 -0.461678 -0.008738 -0.017477   \n",
              "3 -0.010048 -1.199772 -0.747296 -0.040546 -0.461678 -0.008738 -0.017477   \n",
              "4 -0.010048 -0.037666 -0.747296 -0.040546 -0.461678 -0.008738 -0.017477   \n",
              "\n",
              "        66        67        68        69        70        71        72   \\\n",
              "0 -0.739905 -0.432851 -0.564119 -0.049292 -0.058549 -0.190451 -0.008738   \n",
              "1 -0.739905 -0.432851 -0.564119 -0.049292 -0.058549 -0.190451 -0.008738   \n",
              "2 -0.739905 -0.432851 -0.564119 -0.049292 -0.058549 -0.190451 -0.008738   \n",
              "3 -0.739905 -0.432851 -0.564119 -0.049292 -0.058549 -0.190451 -0.008738   \n",
              "4 -0.739905 -0.432851 -0.564119 -0.049292 -0.058549 -0.190451 -0.008738   \n",
              "\n",
              "        73        74        75        76        77        78        79   \\\n",
              "0 -0.025482 -0.069517 -0.008738 -0.128719 -0.037607 -0.536624 -0.008738   \n",
              "1 -0.025482 -0.069517 -0.008738 -0.128719 -0.037607 -0.536624 -0.008738   \n",
              "2 -0.025482 -0.069517 -0.008738 -0.128719 -0.037607 -0.536624 -0.008738   \n",
              "3 -0.025482 -0.069517 -0.008738 -0.128719 -0.037607 -0.536624 -0.008738   \n",
              "4 -0.025482 -0.069517 -0.008738 -0.128719 -0.037607 -0.536624 -0.008738   \n",
              "\n",
              "        80        81        82        83        84       85        86   \\\n",
              "0 -0.008738 -0.057888 -0.039591 -0.013816 -0.017477 -0.14596 -0.013816   \n",
              "1 -0.008738 -0.057888 -0.039591 -0.013816 -0.017477 -0.14596 -0.013816   \n",
              "2 -0.008738 -0.057888 -0.039591 -0.013816 -0.017477 -0.14596 -0.013816   \n",
              "3 -0.008738 -0.057888 -0.039591 -0.013816 -0.017477 -0.14596 -0.013816   \n",
              "4 -0.008738 -0.057888 -0.039591 -0.013816 -0.017477 -0.14596 -0.013816   \n",
              "\n",
              "        87        88        89        90        91       92        93   \\\n",
              "0 -0.108441 -0.259927 -0.011559 -0.017477 -0.015753 -0.06924 -0.008738   \n",
              "1 -0.108441 -0.259927 -0.011559 -0.017477 -0.015753 -0.06924 -0.008738   \n",
              "2 -0.108441 -0.259927 -0.011559 -0.017477 -0.015753 -0.06924 -0.008738   \n",
              "3 -0.108441 -0.259927 -0.011559 -0.017477 -0.015753 -0.06924 -0.008738   \n",
              "4 -0.108441 -0.259927 -0.011559 -0.017477 -0.015753 -0.06924 -0.008738   \n",
              "\n",
              "        94        95        96        97        98        99        100  \\\n",
              "0 -0.013816 -0.313762 -0.008738 -0.020495 -0.019046 -0.033289 -1.226344   \n",
              "1 -0.013816 -0.313762 -0.008738 -0.020495 -0.019046 -0.033289 -1.226344   \n",
              "2 -0.013816 -0.313762 -0.008738 -0.020495 -0.019046 -0.033289 -1.226344   \n",
              "3 -0.013816 -0.313762 -0.008738 -0.020495 -0.019046 -0.033289 -1.226344   \n",
              "4 -0.013816 -0.313762 -0.008738 -0.020495 -0.019046 -0.033289 -1.226344   \n",
              "\n",
              "        101       102       103       104       105      106       107  \\\n",
              "0 -0.287567 -0.176444 -0.229929 -0.055344 -0.035782 -0.05447 -0.128028   \n",
              "1 -0.287567 -0.176444 -0.229929 -0.055344 -0.035782 -0.05447 -0.128028   \n",
              "2 -0.287567 -0.176444 -0.229929 -0.055344 -0.035782 -0.05447 -0.128028   \n",
              "3 -0.287567 -0.176444 -0.229929 -0.055344 -0.035782 -0.05447 -0.128028   \n",
              "4 -0.287567 -0.176444 -0.229929 -0.055344 -0.035782 -0.05447 -0.128028   \n",
              "\n",
              "        108       109       110       111       112       113       114  \\\n",
              "0 -0.011559 -0.160884 -0.013816 -0.020495 -0.017477 -0.419249 -0.268361   \n",
              "1 -0.011559 -0.160884 -0.013816 -0.020495 -0.017477 -0.419249 -0.268361   \n",
              "2 -0.011559 -0.160884 -0.013816 -0.020495 -0.017477 -0.419249 -0.268361   \n",
              "3 -0.011559 -0.160884 -0.013816 -0.020495 -0.017477 -0.419249 -0.268361   \n",
              "4 -0.011559 -0.160884 -0.013816 -0.020495 -0.017477 -0.419249 -0.268361   \n",
              "\n",
              "        115       116       117       118       119       120       121  \\\n",
              "0 -0.145755 -0.040782 -0.111464 -0.231901 -0.440503 -0.198537 -0.554032   \n",
              "1 -0.145755 -0.040782 -0.111464 -0.231901 -0.440503 -0.198537 -0.554032   \n",
              "2 -0.145755 -0.040782 -0.111464 -0.231901 -0.440503 -0.198537 -0.554032   \n",
              "3 -0.145755 -0.040782 -0.111464 -0.231901 -0.440503 -0.198537 -0.554032   \n",
              "4 -0.145755 -0.040782 -0.111464 -0.231901 -0.440503 -0.198537 -0.554032   \n",
              "\n",
              "        122       123       124       125       126       127       128  \\\n",
              "0 -0.020956 -0.011559 -0.102996 -0.021848 -0.079002 -0.123168 -0.011559   \n",
              "1 -0.020956 -0.011559 -0.102996 -0.021848 -0.079002 -0.123168 -0.011559   \n",
              "2 -0.020956 -0.011559 -0.102996 -0.021848 -0.079002 -0.123168 -0.011559   \n",
              "3 -0.020956 -0.011559 -0.102996 -0.021848 -0.079002 -0.123168 -0.011559   \n",
              "4 -0.020956 -0.011559 -0.102996 -0.021848 -0.079002 -0.123168 -0.011559   \n",
              "\n",
              "        129       130       131       132       133       134       135  \\\n",
              "0 -0.008738 -0.070618 -0.008738 -0.024331 -0.205193 -0.167503 -0.222754   \n",
              "1 -0.008738 -0.070618 -0.008738 -0.024331 -0.205193 -0.167503 -0.222754   \n",
              "2 -0.008738 -0.070618 -0.008738 -0.024331 -0.205193 -0.167503 -0.222754   \n",
              "3 -0.008738 -0.070618 -0.008738 -0.024331 -0.205193 -0.167503 -0.222754   \n",
              "4 -0.008738 -0.070618 -0.008738 -0.024331 -0.205193 -0.167503 -0.222754   \n",
              "\n",
              "        136      137       138       139       140       141      142  \\\n",
              "0 -0.062978 -0.17477 -0.581809 -0.288615 -0.284011 -0.049679 -1.36741   \n",
              "1 -0.062978 -0.17477 -0.581809 -0.288615 -0.284011 -0.049679 -1.36741   \n",
              "2 -0.062978 -0.17477 -0.581809 -0.288615 -0.284011 -0.049679 -1.36741   \n",
              "3 -0.062978 -0.17477 -0.581809 -0.288615 -0.284011 -0.049679 -1.36741   \n",
              "4 -0.062978 -0.17477 -0.581809 -0.288615 -0.284011 -0.049679 -1.36741   \n",
              "\n",
              "        143       144       145       146       147       148       149  \\\n",
              "0 -0.088478 -0.060961 -0.055517 -0.071432 -0.015753 -0.008738 -0.047711   \n",
              "1 -0.088478 -0.060961 -0.055517 -0.071432 -0.015753 -0.008738 -0.047711   \n",
              "2 -0.088478 -0.060961 -0.055517 -0.071432 -0.015753 -0.008738 -0.047711   \n",
              "3 -0.088478 -0.060961 -0.055517 -0.071432 -0.015753 -0.008738 -0.047711   \n",
              "4 -0.088478 -0.060961 -0.055517 -0.071432 -0.015753 -0.008738 -0.047711   \n",
              "\n",
              "        150       151      152       153       154       155       156  \\\n",
              "0 -0.605361 -0.045658 -0.22762 -0.182952 -0.011559 -0.008738 -0.092655   \n",
              "1 -0.605361 -0.045658 -0.22762 -0.182952 -0.011559 -0.008738 -0.092655   \n",
              "2 -0.605361 -0.045658 -0.22762 -0.182952 -0.011559 -0.008738 -0.092655   \n",
              "3 -0.605361 -0.045658 -0.22762 -0.182952 -0.011559 -0.008738 -0.092655   \n",
              "4 -0.605361 -0.045658 -0.22762 -0.182952 -0.011559 -0.008738 -0.092655   \n",
              "\n",
              "        157       158       159       160       161     162       163  \\\n",
              "0 -0.708757 -0.354479 -0.034971 -0.008738 -0.173081 -0.1669 -0.075118   \n",
              "1 -0.708757 -0.354479 -0.034971 -0.008738 -0.173081 -0.1669 -0.075118   \n",
              "2 -0.708757 -0.354479 -0.034971 -0.008738 -0.173081 -0.1669 -0.075118   \n",
              "3 -0.708757 -0.354479 -0.034971 -0.008738 -0.173081 -0.1669 -0.075118   \n",
              "4 -0.708757 -0.354479 -0.034971 -0.008738 -0.173081 -0.1669 -0.075118   \n",
              "\n",
              "        164       165       166       167       168       169      170  \\\n",
              "0 -0.036313 -0.008738 -0.035782 -0.016348 -0.008738 -0.441769 -0.02899   \n",
              "1 -0.036313 -0.008738 -0.035782 -0.016348 -0.008738 -0.441769 -0.02899   \n",
              "2 -0.036313 -0.008738 -0.035782 -0.016348 -0.008738 -0.441769 -0.02899   \n",
              "3 -0.036313 -0.008738 -0.035782 -0.016348 -0.008738 -0.441769 -0.02899   \n",
              "4 -0.036313 -0.008738 -0.035782 -0.016348 -0.008738 -0.441769 -0.02899   \n",
              "\n",
              "        171       172       173       174       175       176       177  \\\n",
              "0 -0.008738 -0.017477 -0.131525 -0.054821 -0.034971 -0.062978 -0.020495   \n",
              "1 -0.008738 -0.017477 -0.131525 -0.054821 -0.034971 -0.062978 -0.020495   \n",
              "2 -0.008738 -0.017477 -0.131525 -0.054821 -0.034971 -0.062978 -0.020495   \n",
              "3 -0.008738 -0.017477 -0.131525 -0.054821 -0.034971 -0.062978 -0.020495   \n",
              "4 -0.008738 -0.017477 -0.131525 -0.054821 -0.034971 -0.062978 -0.020495   \n",
              "\n",
              "        178       179       180       181      182       183       184  \\\n",
              "0 -0.008738 -0.322745 -0.210391 -0.099729 -0.28846 -0.341206 -0.019046   \n",
              "1 -0.008738 -0.322745 -0.210391 -0.099729 -0.28846 -0.341206 -0.019046   \n",
              "2 -0.008738 -0.322745 -0.210391 -0.099729 -0.28846 -0.341206 -0.019046   \n",
              "3 -0.008738 -0.322745 -0.210391 -0.099729 -0.28846 -0.341206 -0.019046   \n",
              "4 -0.008738 -0.322745 -0.210391 -0.099729 -0.28846 -0.341206 -0.019046   \n",
              "\n",
              "        185       186       187       188       189       190       191  \\\n",
              "0 -0.126791 -0.149668 -0.075885 -0.122367 -0.008738 -0.349066 -0.128873   \n",
              "1 -0.126791 -0.149668 -0.075885 -0.122367 -0.008738 -0.349066 -0.128873   \n",
              "2 -0.126791 -0.149668 -0.075885 -0.122367 -0.008738 -0.349066 -0.128873   \n",
              "3 -0.126791 -0.149668 -0.075885 -0.122367 -0.008738 -0.349066 -0.128873   \n",
              "4 -0.126791 -0.149668 -0.075885 -0.122367 -0.008738 -0.349066 -0.128873   \n",
              "\n",
              "        192       193      194      195       196       197       198  \\\n",
              "0 -0.386666 -0.421449 -0.14459 -0.13115 -0.289468 -0.008738 -0.017477   \n",
              "1 -0.386666 -0.421449 -0.14459 -0.13115 -0.289468 -0.008738 -0.017477   \n",
              "2 -0.386666 -0.421449 -0.14459 -0.13115 -0.289468 -0.008738 -0.017477   \n",
              "3 -0.386666 -0.421449 -0.14459 -0.13115 -0.289468 -0.008738 -0.017477   \n",
              "4 -0.386666 -0.421449 -0.14459 -0.13115 -0.289468 -0.008738 -0.017477   \n",
              "\n",
              "        199       200       201      202       203       204       205  \\\n",
              "0 -0.008738 -0.061589 -0.452126 -0.02899 -0.042394 -0.102712 -0.064488   \n",
              "1 -0.008738 -0.061589 -0.452126 -0.02899 -0.042394 -0.102712 -0.064488   \n",
              "2 -0.008738 -0.061589 -0.452126 -0.02899 -0.042394 -0.102712 -0.064488   \n",
              "3 -0.008738 -0.061589 -0.452126 -0.02899 -0.042394 -0.102712 -0.064488   \n",
              "4 -0.008738 -0.061589 -0.452126 -0.02899 -0.042394 -0.102712 -0.064488   \n",
              "\n",
              "        206       207       208       209       210       211       212  \\\n",
              "0 -0.232631 -0.110406 -0.035782 -0.600006 -0.051013 -0.008738 -0.936565   \n",
              "1 -0.232631 -0.110406 -0.035782 -0.600006 -0.051013 -0.008738 -0.936565   \n",
              "2 -0.232631 -0.110406 -0.035782 -0.600006 -0.051013 -0.008738 -0.936565   \n",
              "3 -0.232631 -0.110406 -0.035782 -0.600006 -0.051013 -0.008738 -0.936565   \n",
              "4 -0.232631 -0.110406 -0.035782 -0.600006 -0.051013 -0.008738 -0.936565   \n",
              "\n",
              "        213       214       215       216       217       218       219  \\\n",
              "0 -0.022281 -0.064488 -0.261138 -0.024331 -0.047104 -0.168224 -0.047711   \n",
              "1 -0.022281 -0.064488 -0.261138 -0.024331 -0.047104 -0.168224 -0.047711   \n",
              "2 -0.022281 -0.064488 -0.261138 -0.024331 -0.047104 -0.168224 -0.047711   \n",
              "3 -0.022281 -0.064488 -0.261138 -0.024331 -0.047104 -0.168224 -0.047711   \n",
              "4 -0.022281 -0.064488 -0.261138 -0.024331 -0.047104 -0.168224 -0.047711   \n",
              "\n",
              "        220       221       222       223       224       225       226  \\\n",
              "0 -0.210044 -0.037861 -0.011559 -0.017477 -0.080936 -0.747271 -0.079733   \n",
              "1 -0.210044 -0.037861 -0.011559 -0.017477 -0.080936 -0.747271 -0.079733   \n",
              "2 -0.210044 -0.037861 -0.011559 -0.017477 -0.080936 -0.747271 -0.079733   \n",
              "3 -0.210044 -0.037861 -0.011559 -0.017477 -0.080936 -0.747271 -0.079733   \n",
              "4 -0.210044 -0.037861 -0.011559 -0.017477 -0.080936 -0.747271 -0.079733   \n",
              "\n",
              "        227       228       229       230       231       232       233  \\\n",
              "0 -0.396779 -0.039591 -0.364666 -0.065377 -0.801457 -0.431047 -0.209052   \n",
              "1 -0.396779 -0.039591 -0.364666 -0.065377 -0.801457 -0.431047 -0.209052   \n",
              "2 -0.396779 -0.039591 -0.364666 -0.065377 -0.801457 -0.431047 -0.209052   \n",
              "3 -0.396779 -0.039591 -0.364666 -0.065377 -0.801457 -0.431047 -0.209052   \n",
              "4 -0.396779 -0.039591 -0.364666 -0.065377 -0.801457 -0.431047 -0.209052   \n",
              "\n",
              "        234       235      236       237       238       239       240  \\\n",
              "0 -0.037352 -0.008738 -0.07356 -0.049292 -0.030595 -0.042619 -0.167443   \n",
              "1 -0.037352 -0.008738 -0.07356 -0.049292 -0.030595 -0.042619 -0.167443   \n",
              "2 -0.037352 -0.008738 -0.07356 -0.049292 -0.030595 -0.042619 -0.167443   \n",
              "3 -0.037352 -0.008738 -0.07356 -0.049292 -0.030595 -0.042619 -0.167443   \n",
              "4 -0.037352 -0.008738 -0.07356 -0.049292 -0.030595 -0.042619 -0.167443   \n",
              "\n",
              "        241       242       243       244       245       246       247  \\\n",
              "0 -0.121156 -0.008738 -0.011559 -0.144728 -1.328428 -0.375136 -0.526081   \n",
              "1 -0.121156 -0.008738 -0.011559 -0.144728 -1.328428 -0.375136 -0.526081   \n",
              "2 -0.121156 -0.008738 -0.011559 -0.144728 -1.328428 -0.375136 -0.526081   \n",
              "3 -0.121156 -0.008738 -0.011559 -0.144728 -1.328428 -0.375136 -0.526081   \n",
              "4 -0.121156 -0.008738 -0.011559 -0.144728 -1.328428 -0.375136 -0.526081   \n",
              "\n",
              "        248       249       250       251       252      253       254  \\\n",
              "0 -0.088149 -0.008738 -0.123884 -0.080216 -0.013816 -0.16324 -0.122045   \n",
              "1 -0.088149 -0.008738 -0.123884 -0.080216 -0.013816 -0.16324 -0.122045   \n",
              "2 -0.088149 -0.008738 -0.123884 -0.080216 -0.013816 -0.16324 -0.122045   \n",
              "3 -0.088149 -0.008738 -0.123884 -0.080216 -0.013816 -0.16324 -0.122045   \n",
              "4 -0.088149 -0.008738 -0.123884 -0.080216 -0.013816 -0.16324 -0.122045   \n",
              "\n",
              "        255       256       257       258       259       260       261  \\\n",
              "0 -0.015753 -0.209499 -0.047911 -0.058712 -0.051201 -0.092341 -0.017477   \n",
              "1 -0.015753 -0.209499 -0.047911 -0.058712 -0.051201 -0.092341 -0.017477   \n",
              "2 -0.015753 -0.209499 -0.047911 -0.058712 -0.051201 -0.092341 -0.017477   \n",
              "3 -0.015753 -0.209499 -0.047911 -0.058712 -0.051201 -0.092341 -0.017477   \n",
              "4 -0.015753 -0.209499 -0.047911 -0.058712 -0.051201 -0.092341 -0.017477   \n",
              "\n",
              "        262       263       264       265       266       267       268  \\\n",
              "0 -0.358559 -0.361906 -0.400283 -0.445005  2.227325 -0.439113 -0.399224   \n",
              "1 -0.358559 -0.361906 -0.400283 -0.445005  2.227325 -0.439113 -0.399224   \n",
              "2 -0.358559 -0.361906 -0.400283 -0.445005  2.227325 -0.439113 -0.399224   \n",
              "3 -0.358559 -0.361906 -0.400283 -0.445005  2.227325 -0.439113 -0.399224   \n",
              "4 -0.358559 -0.361906 -0.400283 -0.445005 -0.448969  2.277320 -0.399224   \n",
              "\n",
              "       269       270       271       272       273       274       275  \\\n",
              "0 -0.43518 -0.448846 -0.454696  2.272362 -0.427399 -0.326158 -0.312402   \n",
              "1 -0.43518 -0.448846  2.199274 -0.440071 -0.427399 -0.326158 -0.312402   \n",
              "2 -0.43518 -0.448846  2.199274 -0.440071 -0.427399 -0.326158 -0.312402   \n",
              "3 -0.43518 -0.448846 -0.454696  2.272362 -0.427399 -0.326158 -0.312402   \n",
              "4 -0.43518 -0.448846 -0.454696  2.272362 -0.427399 -0.326158 -0.312402   \n",
              "\n",
              "       276  277       278  279   280  \n",
              "0 -0.08286  0.0 -0.045171  0.0   1.0  \n",
              "1 -0.08286  0.0 -0.045171  0.0   0.0  \n",
              "2 -0.08286  0.0 -0.045171  0.0   0.0  \n",
              "3 -0.08286  0.0 -0.045171  0.0   1.0  \n",
              "4 -0.08286  0.0 -0.045171  0.0  27.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "LNpay8_hShny",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = blog_df.drop(columns=280)\n",
        "y = blog_df[280]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CMI8C8OnJmMY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3dc3ee1a-db47-45c1-a50c-7e9ae967807a"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)\n",
        "lin_reg_split = LinearRegression().fit(X_train, y_train)\n",
        "print(mean_squared_error(y, lin_reg_split.predict(X)))\n",
        "print(mean_squared_error(y_test, lin_reg_split.predict(X_test)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "904.8541816435529\n",
            "797.002228491887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "a5d7c8f0-7e46-402e-bce4-383f80603553",
        "id": "HpyQQwQwMOX0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1774
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "alphas = []\n",
        "mses = []\n",
        "\n",
        "for alpha in range(0, 100, 1):\n",
        "  ridge_reg_split = Ridge(alpha=alpha).fit(X_train, y_train)\n",
        "  mse = mean_squared_error(y_test, ridge_reg_split.predict(X_test))\n",
        "  alphas.append(alpha)\n",
        "  mses.append(mse)\n",
        "  print(alpha, mse)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 798.4458881072518\n",
            "1 797.0467673188679\n",
            "2 797.0171164876361\n",
            "3 796.99691112172\n",
            "4 796.9813499235544\n",
            "5 796.9689708739721\n",
            "6 796.9589769334375\n",
            "7 796.9508390838203\n",
            "8 796.9441798191306\n",
            "9 796.938719757166\n",
            "10 796.9342470071033\n",
            "11 796.9305976990144\n",
            "12 796.9276429541511\n",
            "13 796.9252798874271\n",
            "14 796.9234252506993\n",
            "15 796.9220108569068\n",
            "16 796.9209802317775\n",
            "17 796.920286127208\n",
            "18 796.919888649193\n",
            "19 796.9197538304297\n",
            "20 796.919852528998\n",
            "21 796.9201595691325\n",
            "22 796.9206530638392\n",
            "23 796.9213138756173\n",
            "24 796.9221251831423\n",
            "25 796.923072130061\n",
            "26 796.9241415380017\n",
            "27 796.9253216702491\n",
            "28 796.9266020357236\n",
            "29 796.9279732252842\n",
            "30 796.9294267741398\n",
            "31 796.9309550455101\n",
            "32 796.9325511316933\n",
            "33 796.9342087694776\n",
            "34 796.9359222674664\n",
            "35 796.9376864433287\n",
            "36 796.9394965693866\n",
            "37 796.9413483252282\n",
            "38 796.9432377562807\n",
            "39 796.9451612374511\n",
            "40 796.9471154411115\n",
            "41 796.9490973088089\n",
            "42 796.9511040261891\n",
            "43 796.9531330007122\n",
            "44 796.9551818417775\n",
            "45 796.9572483429652\n",
            "46 796.9593304661246\n",
            "47 796.9614263270789\n",
            "48 796.9635341827623\n",
            "49 796.9656524196091\n",
            "50 796.967779543068\n",
            "51 796.9699141680961\n",
            "52 796.972055010543\n",
            "53 796.9742008793111\n",
            "54 796.9763506692267\n",
            "55 796.978503354534\n",
            "56 796.9806579829565\n",
            "57 796.9828136702654\n",
            "58 796.9849695953064\n",
            "59 796.987124995438\n",
            "60 796.9892791623437\n",
            "61 796.991431438185\n",
            "62 796.9935812120572\n",
            "63 796.9957279167264\n",
            "64 796.9978710256207\n",
            "65 797.000010050051\n",
            "66 797.0021445366431\n",
            "67 797.004274064963\n",
            "68 797.006398245318\n",
            "69 797.0085167167189\n",
            "70 797.0106291449919\n",
            "71 797.0127352210222\n",
            "72 797.0148346591276\n",
            "73 797.0169271955432\n",
            "74 797.0190125870124\n",
            "75 797.0210906094765\n",
            "76 797.0231610568542\n",
            "77 797.0252237398997\n",
            "78 797.0272784851454\n",
            "79 797.029325133909\n",
            "80 797.0313635413668\n",
            "81 797.033393575691\n",
            "82 797.0354151172419\n",
            "83 797.0374280578102\n",
            "84 797.0394322999117\n",
            "85 797.0414277561239\n",
            "86 797.0434143484657\n",
            "87 797.0453920078166\n",
            "88 797.0473606733716\n",
            "89 797.0493202921296\n",
            "90 797.0512708184145\n",
            "91 797.0532122134255\n",
            "92 797.0551444448124\n",
            "93 797.0570674862807\n",
            "94 797.0589813172168\n",
            "95 797.0608859223383\n",
            "96 797.0627812913638\n",
            "97 797.0646674187029\n",
            "98 797.0665443031635\n",
            "99 797.0684119476774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UzePj6zwNMvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "f51ad83d-ebb0-4135-efe0-7aeb5a5ebc20"
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize = (12, 10))\n",
        "plt.scatter(alphas, mses, color = 'red', alpha = 0.5)\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAJCCAYAAAACzkvXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAwDAoP8PZi1tshiRAZUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LhewyLpFh6sV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample_df = blog_df.sample(10000, random_state=43)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BYUAVglyiJfW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = sample_df.drop(columns=280)\n",
        "y = sample_df[280]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbAnPtPEiOe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f39430f3-a0e3-4506-b911-ff41d7c99b51"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)\n",
        "lin_reg_split = LinearRegression().fit(X_train, y_train)\n",
        "print(mean_squared_error(y, lin_reg_split.predict(X)))\n",
        "print(mean_squared_error(y_test, lin_reg_split.predict(X_test)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4911504550603886e+24\n",
            "9.964601820241554e+24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "22kF5Z9RiRxp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1757
        },
        "outputId": "c2e25424-fd48-4e61-f8c0-1cd6daa88348"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "alphas = []\n",
        "mses = []\n",
        "\n",
        "for alpha in range(1, 100, 1):\n",
        "  ridge_reg_split = Ridge(alpha=alpha).fit(X_train, y_train)\n",
        "  mse = mean_squared_error(y_test, ridge_reg_split.predict(X_test))\n",
        "  alphas.append(alpha)\n",
        "  mses.append(mse)\n",
        "  print(alpha, mse)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 584.282283309788\n",
            "2 584.1241194963872\n",
            "3 583.9874279393586\n",
            "4 583.8835353297407\n",
            "5 583.8056974025187\n",
            "6 583.7475506064492\n",
            "7 583.70437033189\n",
            "8 583.6727367153695\n",
            "9 583.6501491216527\n",
            "10 583.6347468421435\n",
            "11 583.6251202371516\n",
            "12 583.620183490986\n",
            "13 583.6190876842683\n",
            "14 583.6211602982235\n",
            "15 583.6258623040113\n",
            "16 583.6327571689578\n",
            "17 583.64148809273\n",
            "18 583.6517610312635\n",
            "19 583.6633318604217\n",
            "20 583.6759965471507\n",
            "21 583.6895835370112\n",
            "22 583.7039477965865\n",
            "23 583.7189661063898\n",
            "24 583.7345333091723\n",
            "25 583.7505592955669\n",
            "26 583.766966564121\n",
            "27 583.7836882326327\n",
            "28 583.8006664069462\n",
            "29 583.817850834964\n",
            "30 583.8351977898291\n",
            "31 583.8526691384288\n",
            "32 583.870231560672\n",
            "33 583.8878558921343\n",
            "34 583.905516568184\n",
            "35 583.9231911520038\n",
            "36 583.9408599323076\n",
            "37 583.9585055792095\n",
            "38 583.9761128488218\n",
            "39 583.9936683288544\n",
            "40 584.0111602188422\n",
            "41 584.0285781397283\n",
            "42 584.0459129684143\n",
            "43 584.0631566936221\n",
            "44 584.0803022900026\n",
            "45 584.0973436079092\n",
            "46 584.1142752766705\n",
            "47 584.1310926195126\n",
            "48 584.1477915785755\n",
            "49 584.1643686486905\n",
            "50 584.1808208187807\n",
            "51 584.1971455199147\n",
            "52 584.213340579172\n",
            "53 584.2294041786063\n",
            "54 584.2453348186796\n",
            "55 584.2611312856338\n",
            "56 584.2767926223297\n",
            "57 584.2923181021514\n",
            "58 584.3077072056191\n",
            "59 584.322959599407\n",
            "60 584.3380751174901\n",
            "61 584.353053744191\n",
            "62 584.3678955989117\n",
            "63 584.3826009223756\n",
            "64 584.3971700642094\n",
            "65 584.411603471735\n",
            "66 584.4259016798322\n",
            "67 584.4400653017732\n",
            "68 584.4540950209209\n",
            "69 584.4679915832124\n",
            "70 584.481755790339\n",
            "71 584.4953884935627\n",
            "72 584.5088905881029\n",
            "73 584.52226300804\n",
            "74 584.5355067216802\n",
            "75 584.5486227273484\n",
            "76 584.5616120495583\n",
            "77 584.5744757355309\n",
            "78 584.5872148520265\n",
            "79 584.5998304824628\n",
            "80 584.6123237242904\n",
            "81 584.624695686606\n",
            "82 584.636947487979\n",
            "83 584.6490802544752\n",
            "84 584.6610951178553\n",
            "85 584.672993213941\n",
            "86 584.6847756811243\n",
            "87 584.6964436590149\n",
            "88 584.7079982872121\n",
            "89 584.719440704187\n",
            "90 584.7307720462711\n",
            "91 584.7419934467363\n",
            "92 584.7531060349656\n",
            "93 584.7641109357\n",
            "94 584.775009268358\n",
            "95 584.785802146425\n",
            "96 584.7964906768991\n",
            "97 584.8070759597969\n",
            "98 584.8175590877084\n",
            "99 584.8279411454004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MhYlwbC8iUHF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "7a9a1b8e-c232-4d23-cf6c-30176873ca4b"
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize = (12, 10))\n",
        "plt.scatter(alphas, mses, color = 'red', alpha = 0.5)\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAJCCAYAAAACzkvXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0wAAMKH/HzQF59GODsKWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "sBbzYgAXFX7v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Did you find cases where Ridge performed better? If so, describe (alpha parameter, sample size, any other relevant info/processing). If not, what do you think that tells you about the data\n",
        "\n",
        "**Answer:** For lower sample values Ridge Regression performs better than Linear Regression.\n",
        "\n",
        "Alpha value = 13,\n",
        "Sample Size = 10000 \n",
        "Randome State = 43\n"
      ]
    },
    {
      "metadata": {
        "id": "Onsn4B2tJ20X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Resources and stretch goals"
      ]
    },
    {
      "metadata": {
        "id": "o_ZIP6O0J435",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Resources:\n",
        "- https://www.quora.com/What-is-regularization-in-machine-learning\n",
        "- https://blogs.sas.com/content/subconsciousmusings/2017/07/06/how-to-use-regularization-to-prevent-model-overfitting/\n",
        "- https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\n",
        "- https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
        "- https://stats.stackexchange.com/questions/111017/question-about-standardizing-in-ridge-regression#111022\n",
        "\n",
        "Stretch goals:\n",
        "- Revisit past data you've fit OLS models to, and see if there's an `alpha` such that ridge regression results in a model with lower MSE on a train/test split\n",
        "- Yes, Ridge can be applied to classification! Check out [sklearn.linear_model.RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier), and try it on a problem you previous approached with a different classifier (note - scikit LogisticRegression also automatically penalizes based on the $L^2$ norm, so the difference won't be as dramatic)\n",
        "- Implement your own function to calculate the full cost that ridge regression is optimizing (the sum of squared residuals + `alpha` times the sum of squared coefficients) - this alone won't fit a model, but you can use it to verify cost of trained models and that the coefficients from the equivalent OLS (without regularization) may have a higher cost"
      ]
    }
  ]
}